{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Описание-проекта\" data-toc-modified-id=\"Описание-проекта-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Описание проекта</a></span></li><li><span><a href=\"#Скачивание-и-импорт-библиотек\" data-toc-modified-id=\"Скачивание-и-импорт-библиотек-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Скачивание и импорт библиотек</a></span></li><li><span><a href=\"#Анализ-данных\" data-toc-modified-id=\"Анализ-данных-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Анализ данных</a></span><ul class=\"toc-item\"><li><span><a href=\"#Выделение-обучающей-и-тестовой-выборок\" data-toc-modified-id=\"Выделение-обучающей-и-тестовой-выборок-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Выделение обучающей и тестовой выборок</a></span></li><li><span><a href=\"#Удаление-слов-нейтральной-тональности\" data-toc-modified-id=\"Удаление-слов-нейтральной-тональности-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Удаление слов нейтральной тональности</a></span></li></ul></li><li><span><a href=\"#Векторизация-текста-и-обучение-моделей\" data-toc-modified-id=\"Векторизация-текста-и-обучение-моделей-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Векторизация текста и обучение моделей</a></span><ul class=\"toc-item\"><li><span><a href=\"#Поиск-наилучших-гиперпараметров-при-помощи-GridSearchCV\" data-toc-modified-id=\"Поиск-наилучших-гиперпараметров-при-помощи-GridSearchCV-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Поиск наилучших гиперпараметров при помощи GridSearchCV</a></span></li><li><span><a href=\"#Поиск-наилучших-гиперпараметров-при-помощи-Optuna\" data-toc-modified-id=\"Поиск-наилучших-гиперпараметров-при-помощи-Optuna-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Поиск наилучших гиперпараметров при помощи Optuna</a></span></li></ul></li><li><span><a href=\"#Вывод\" data-toc-modified-id=\"Вывод-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Вывод</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Проект для «Викишоп»***\n",
    "\n",
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "Постройте модель со значением метрики качества F1 не меньше 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Скачивание и импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"{sys.executable}\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import geograpy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import RandomSampler\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import sklearn\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.exceptions import FitFailedWarning, ConvergenceWarning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm  import SVC\n",
    "from sklearn.utils._testing import ignore_warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Zakhar\n",
      "[nltk_data]     Kousnetsov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Zakhar\n",
      "[nltk_data]     Kousnetsov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Zakhar\n",
      "[nltk_data]     Kousnetsov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv('toxic_comments.csv')\n",
    "except:\n",
    "    data = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на содержание датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8552</th>\n",
       "      <td>Javier Patiño \\n\\nNo warnings yet, but the table you were fighting over is unrefernced so I've removed it completely. Snowman</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111801</th>\n",
       "      <td>\"\\n\\nYou deleted by addition to the \"\"miscarriage of justice\"\" page. This is a legitimate site and link, and is not spam. It's a valid organization dedicated to fixing wrongful convictions of Kevin Thornton and others, and is used frequently by the Innocence Project and others. Please re-insert.\\n\\n\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63457</th>\n",
       "      <td>\"\\nTry every single time I type something.... unless you have selective reading as well. The palegoldenrod is used in the header as well, not just in conflict with the yellow.  – (talk) \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                 text  \\\n",
       "8552                                                                                                                                                                                    Javier Patiño \\n\\nNo warnings yet, but the table you were fighting over is unrefernced so I've removed it completely. Snowman   \n",
       "111801  \"\\n\\nYou deleted by addition to the \"\"miscarriage of justice\"\" page. This is a legitimate site and link, and is not spam. It's a valid organization dedicated to fixing wrongful convictions of Kevin Thornton and others, and is used frequently by the Innocence Project and others. Please re-insert.\\n\\n\"   \n",
       "63457                                                                                                                     \"\\nTry every single time I type something.... unless you have selective reading as well. The palegoldenrod is used in the header as well, not just in conflict with the yellow.  – (talk) \"   \n",
       "\n",
       "        toxic  \n",
       "8552        0  \n",
       "111801      0  \n",
       "63457       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В датасете содержится некоторое количество твитов с присвоенной каждому оценкой тональности: токсичный или нет. Посмотрим на общую информацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всего в датасете 159571 твит, каждому присвоена оценка, пропусков нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исследуем дисбаланс классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.898321\n",
       "1    0.101679\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['toxic'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целевом признаке прослеживается сильный дисбаланс классов. При обучении нужно будет принять меры по его компенсации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим наличие в тексте неинформативных элементов: хэштегов, ip-адресов, цифр. Выведем их количество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>153241</th>\n",
       "      <td>\"\\n\\n Korean article template \\n\\nHi, thanks for your question.  Sorry I'm so late getting back to you.  Life on and off Wikipedia has been full of all sorts of unexpected hecticity...  Anyway, the Korean article template. \\n\\nThe idea behind the template is pretty simple.  The basic idea is that every article should have the following, unless there is some reason to the contrary:\\n a relevant \"\"See also\"\" section, \\n at least one relevant Korea-related category, and\\n a name table.  \\n Template:Korean on the Talk page.\\n\\nIn more detail...\\nThe \"\"See also\"\" section should include a link to the List of Korea-related topics  (LKRT), and anything else that's obviously relevant but not linked in the article text.  If you prefer, you can just put in the LKRT link and let someone else add the others.  If you want to add more, here's the rule I've been following:  look in each category of which the article is a member.  If that category has a main article (like Korea in Category:Korea), then that main article should be linked  either from the text, or from the See also section.  The same for parent categories, within reason.  Here's an example:  Munmu of Silla is a member of the categories Silla and Korean rulers.  Thus, the articles Silla and Rulers of Korea are linked in the article text.  Those two categories are part of Category:Korean history and Category:Korean people.  Thus, History of Korea is linked in the See also section, and so is List of Koreans (well, it is now).   Both of those are members of Category:Korea, hence the link to the LKRT.  \\nYou can find a complete (?) list of Korea-related categories at List of Korea-related topics#Categories.  Or just start at Category:Korea and work your way down.\\nTemplate:Koreanname noimage is fine for the vast majority of articles.  Cut and paste Template talk:Koreanname noimage#Syntax and fill in the blanks.  All Korean name tables require you to specify at least these four variables: hangul, hanja, rr, and mr.  \"\"Rr\"\" stands for Revised Romanization of Korean, and \"\"mr\"\" stands for McCune-Reischauer Romanization.  If you don't know what should go in these fields, just leave them blank  however, at a minimum the hangul should be provided (that allows other editors to verify the article and identify the other name-related information).  You can request help on the Korea-related notice board.  \\nThis one's easy.  Just paste {{Korean}} onto the top of the talk page.  Done!\\n\\nMost of our existing articles should already conform to this, since we just finished a housekeeping sweep in June.  But there are more new ones coming in every day...\\n\\nI've also taken to adding a \"\"Wish list\"\" to the talk pages of new/inactive articles, identifying 2 or 3 things that the article is lacking.  Hopefully this will help other contributors see where they can help.   (Of course, if I can contribute the information myself, I do so).\\n\\nI hope this isn't all too overwhelming.  Your excellent work so far is much appreciated. \\n\\nCheers,\\n\\n \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
       "153241  \"\\n\\n Korean article template \\n\\nHi, thanks for your question.  Sorry I'm so late getting back to you.  Life on and off Wikipedia has been full of all sorts of unexpected hecticity...  Anyway, the Korean article template. \\n\\nThe idea behind the template is pretty simple.  The basic idea is that every article should have the following, unless there is some reason to the contrary:\\n a relevant \"\"See also\"\" section, \\n at least one relevant Korea-related category, and\\n a name table.  \\n Template:Korean on the Talk page.\\n\\nIn more detail...\\nThe \"\"See also\"\" section should include a link to the List of Korea-related topics  (LKRT), and anything else that's obviously relevant but not linked in the article text.  If you prefer, you can just put in the LKRT link and let someone else add the others.  If you want to add more, here's the rule I've been following:  look in each category of which the article is a member.  If that category has a main article (like Korea in Category:Korea), then that main article should be linked  either from the text, or from the See also section.  The same for parent categories, within reason.  Here's an example:  Munmu of Silla is a member of the categories Silla and Korean rulers.  Thus, the articles Silla and Rulers of Korea are linked in the article text.  Those two categories are part of Category:Korean history and Category:Korean people.  Thus, History of Korea is linked in the See also section, and so is List of Koreans (well, it is now).   Both of those are members of Category:Korea, hence the link to the LKRT.  \\nYou can find a complete (?) list of Korea-related categories at List of Korea-related topics#Categories.  Or just start at Category:Korea and work your way down.\\nTemplate:Koreanname noimage is fine for the vast majority of articles.  Cut and paste Template talk:Koreanname noimage#Syntax and fill in the blanks.  All Korean name tables require you to specify at least these four variables: hangul, hanja, rr, and mr.  \"\"Rr\"\" stands for Revised Romanization of Korean, and \"\"mr\"\" stands for McCune-Reischauer Romanization.  If you don't know what should go in these fields, just leave them blank  however, at a minimum the hangul should be provided (that allows other editors to verify the article and identify the other name-related information).  You can request help on the Korea-related notice board.  \\nThis one's easy.  Just paste {{Korean}} onto the top of the talk page.  Done!\\n\\nMost of our existing articles should already conform to this, since we just finished a housekeeping sweep in June.  But there are more new ones coming in every day...\\n\\nI've also taken to adding a \"\"Wish list\"\" to the talk pages of new/inactive articles, identifying 2 or 3 things that the article is lacking.  Hopefully this will help other contributors see where they can help.   (Of course, if I can contribute the information myself, I do so).\\n\\nI hope this isn't all too overwhelming.  Your excellent work so far is much appreciated. \\n\\nCheers,\\n\\n \"   \n",
       "\n",
       "        toxic  \n",
       "153241      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего в корпусе 3276 текстов с хэштегами\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105483</th>\n",
       "      <td>\"Christiania ==\\n\\nflag&gt; http://www.fotw.net/images/d/dk-chris.gif\\nby Edward Mooney, Jr. 2000-12-08\\n\\ninfo&gt;    * you limit your use to a maximum of 5% of the images or content of the website\\n    * you quote the author\\n    * you quote the website (as \"\"FOTW Flags Of The World website at http://flagspot.net/flags/\"\")\\n    * you do not alter in any way the images or the content of the text\\n    * you use the material for non-commercial and non-political purposes only\\n    * if you distribute our material by a non-Internet way (e.g., floppy disks or CD-ROM) you must add this copyright text on every copy of the medium\\n    * if you distribute our materials by a non-Internet way (e.g., floppy disks or CD-ROM) you cannot sell these media\\n    * if you want to mirror the Web, read these additional rules\\n    * If you want to reuse FOTW GeoIndex maps and boundaries' data, read these rules \\n\\n \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           text  \\\n",
       "105483  \"Christiania ==\\n\\nflag> http://www.fotw.net/images/d/dk-chris.gif\\nby Edward Mooney, Jr. 2000-12-08\\n\\ninfo>    * you limit your use to a maximum of 5% of the images or content of the website\\n    * you quote the author\\n    * you quote the website (as \"\"FOTW Flags Of The World website at http://flagspot.net/flags/\"\")\\n    * you do not alter in any way the images or the content of the text\\n    * you use the material for non-commercial and non-political purposes only\\n    * if you distribute our material by a non-Internet way (e.g., floppy disks or CD-ROM) you must add this copyright text on every copy of the medium\\n    * if you distribute our materials by a non-Internet way (e.g., floppy disks or CD-ROM) you cannot sell these media\\n    * if you want to mirror the Web, read these additional rules\\n    * If you want to reuse FOTW GeoIndex maps and boundaries' data, read these rules \\n\\n \"   \n",
       "\n",
       "        toxic  \n",
       "105483      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего в корпусе 4774 текстов с названиями сайтов\n"
     ]
    }
   ],
   "source": [
    "display(data[data['text'].str.contains(r'#\\S+')].sample())\n",
    "print('Всего в корпусе {} текстов с хэштегами'.format(data['text'].str.contains('#\\S+').sum()))\n",
    "\n",
    "display(data[data['text'].str.contains('http[s]?\\://\\S+')].sample())\n",
    "print('Всего в корпусе {} текстов с названиями сайтов'.format(data['text'].str.contains('http[s]?\\://\\S+').sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очистим все твиты от неинформативных символов и приведем все слова к леммам при помощи библиотеки `spaCy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "def cleaner_lemmatizer(text):\n",
    "    cleaned_text = ' '.join(re.sub(r'(#\\S+)|(http[s]?\\://\\S+)|[^a-zA-Z]', ' ', text).split())\n",
    "    return ' '.join([token.lemma_ for token in nlp(cleaned_text)]).lower().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся, что это работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>comment i could not verify the claim talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>you be wrong although i be block on some occasion i be never block for sockpuppetry or vandalism it be because of dispute on multiple topic with various sock of user hkelkar user vandalpetrol both of which be permabanne i have never use any other account if you be permit to edit then you should get other account delete which say you be indeffe no blast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>unsure at this stage its not very clear what wording be actually be discuss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                   text\n",
       "111                                                                                                                                                                                                                                                                                                                           comment i could not verify the claim talk\n",
       "222  you be wrong although i be block on some occasion i be never block for sockpuppetry or vandalism it be because of dispute on multiple topic with various sock of user hkelkar user vandalpetrol both of which be permabanne i have never use any other account if you be permit to edit then you should get other account delete which say you be indeffe no blast\n",
       "333                                                                                                                                                                                                                                                                                         unsure at this stage its not very clear what wording be actually be discuss"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[[111, 222, 333], 'text'].apply(func=lambda x: cleaner_lemmatizer(x)).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сэкономить время, загрузим датасет с уже очищенными и лемматизированными твитами "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    data = pd.read_csv('data_cleaned_and lemmatized_v2').drop('Unnamed: 0', axis=1)\n",
    "except:\n",
    "    data['lemmatized'] = data.loc[:, 'text'].apply(func=lambda x: cleaner_lemmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47633</th>\n",
       "      <td>\"\\nNon-free images are strictly limited to certain extremely limited situations where obtaining a replacement is impossible - see WP:NFC. Non-replaceable does not simply mean \"\"not easily available on the Internet\"\" - there are plenty of opportunities to take a free-content picture of a transman for the article. ✽ \"</td>\n",
       "      <td>0</td>\n",
       "      <td>non free image be strictly limit to certain extremely limited situation where obtain a replacement be impossible see wp nfc non replaceable do not simply mean not easily available on the internet there be plenty of opportunity to take a free content picture of a transman for the article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85234</th>\n",
       "      <td>It seems that it significantly informed her understanding of the use of ICTs and motivated her interest in them.  Admittedly, I don't know offhand of any notable interviews, articles, etc. outside of danah's self-published material that makes this connection.  I personally believe that the media have totally missed this important and interesting connection but that's beyond the scope of this discussion (and a significant weakness in this particular argument).</td>\n",
       "      <td>0</td>\n",
       "      <td>it seem that it significantly inform her understanding of the use of ict and motivate her interest in they admittedly i don t know offhand of any notable interview article etc outside of danah s self publish material that make this connection i personally believe that the medium have totally miss this important and interesting connection but that s beyond the scope of this discussion and a significant weakness in this particular argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131847</th>\n",
       "      <td>I did, he just called Tiger a racist, called the black communities racist and called him a failure at golf. Doesnt mean hes a racist.</td>\n",
       "      <td>1</td>\n",
       "      <td>i do he just call tiger a racist call the black community racist and call he a failure at golf do not mean he s a racist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text  \\\n",
       "47633                                                                                                                                                     \"\\nNon-free images are strictly limited to certain extremely limited situations where obtaining a replacement is impossible - see WP:NFC. Non-replaceable does not simply mean \"\"not easily available on the Internet\"\" - there are plenty of opportunities to take a free-content picture of a transman for the article. ✽ \"   \n",
       "85234   It seems that it significantly informed her understanding of the use of ICTs and motivated her interest in them.  Admittedly, I don't know offhand of any notable interviews, articles, etc. outside of danah's self-published material that makes this connection.  I personally believe that the media have totally missed this important and interesting connection but that's beyond the scope of this discussion (and a significant weakness in this particular argument).   \n",
       "131847                                                                                                                                                                                                                                                                                                                                            I did, he just called Tiger a racist, called the black communities racist and called him a failure at golf. Doesnt mean hes a racist.   \n",
       "\n",
       "        toxic  \\\n",
       "47633       0   \n",
       "85234       0   \n",
       "131847      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                       lemmatized  \n",
       "47633                                                                                                                                                             non free image be strictly limit to certain extremely limited situation where obtain a replacement be impossible see wp nfc non replaceable do not simply mean not easily available on the internet there be plenty of opportunity to take a free content picture of a transman for the article  \n",
       "85234   it seem that it significantly inform her understanding of the use of ict and motivate her interest in they admittedly i don t know offhand of any notable interview article etc outside of danah s self publish material that make this connection i personally believe that the medium have totally miss this important and interesting connection but that s beyond the scope of this discussion and a significant weakness in this particular argument  \n",
       "131847                                                                                                                                                                                                                                                                                                                                   i do he just call tiger a racist call the black community racist and call he a failure at golf do not mean he s a racist  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим сводную информацию о датасете вновь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   text        159571 non-null  object\n",
      " 1   toxic       159571 non-null  int64 \n",
      " 2   lemmatized  159548 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В лемматазированных текстах появились пропуски. Рассмотрим их отдельно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76037</th>\n",
       "      <td>http://finance.yahoo.com/news/7-fascinating-nuggets-another-bewildering-150348488.html</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141293</th>\n",
       "      <td>http://www.haaretz.com/news/diplomacy-defense/2-279-calories-per-person-how-israel-made-sure-gaza-didn-t-starve.premium-1.470419</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153616</th>\n",
       "      <td>http://en.wikipedia.org/w/index.php?title=Crash_Bandicoot_%28series%29&amp;diff;=prev&amp;oldid;=484470162]], [], [], [], [] [[</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17311</th>\n",
       "      <td>~ \\n\\n68.193.147.157</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109944</th>\n",
       "      <td>http://www.newswire.ca/fr/story/358197/domaine-pinnacle-ice-cider-now-available-in-new-european-and-asian-markets .</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                    text  \\\n",
       "76037                                             http://finance.yahoo.com/news/7-fascinating-nuggets-another-bewildering-150348488.html   \n",
       "141293  http://www.haaretz.com/news/diplomacy-defense/2-279-calories-per-person-how-israel-made-sure-gaza-didn-t-starve.premium-1.470419   \n",
       "153616           http://en.wikipedia.org/w/index.php?title=Crash_Bandicoot_%28series%29&diff;=prev&oldid;=484470162]], [], [], [], [] [[   \n",
       "17311                                                                                                               ~ \\n\\n68.193.147.157   \n",
       "109944               http://www.newswire.ca/fr/story/358197/domaine-pinnacle-ice-cider-now-available-in-new-european-and-asian-markets .   \n",
       "\n",
       "        toxic lemmatized  \n",
       "76037       0        NaN  \n",
       "141293      0        NaN  \n",
       "153616      0        NaN  \n",
       "17311       0        NaN  \n",
       "109944      0        NaN  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['lemmatized'].isna()].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все ясно, пропуски связаны с тем, что в оригинальном твите нет текста и лемматизировать нечего. Придется такие наблюдения удалить, так как вряд ли нам удастся вычислить настроение по IP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для анализа дан довольно большой корпус текста. Для точности обучения данных чем больше, тем лучше, но это может потребовать существенных вычислительных ресурсов, поэтому проверим, не перестает ли увеличиваться число уникальных слов в твитах, начиная  с некоторого значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.16 s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8YAAAGDCAYAAADpmwk3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAB6oklEQVR4nO3dd5xddZ3/8dcnPSF1UkjvJEAoIY0OQRAQUJRFQFGK2F1FXbGs7q66dnfd36q7KiJVBLMIgvQiCEh6IXRII52USa9TPr8/zplwM0y5ycx87j133s/HI4/MnFvmO697Ur5zzvdcc3dEREREREREWqs2hR6AiIiIiIiISCFpYiwiIiIiIiKtmibGIiIiIiIi0qppYiwiIiIiIiKtmibGIiIiIiIi0qppYiwiIiIiIiKtmibGIiIiIiIi0qppYizSCpnZVWZWZWbb01+7zezZQo9LRERERKQQNDEWab2mu3tXd+8KfLrQgxERERERKRRNjEVap/ZAVX03pkeUn835/Ktm5mZ2Vvr5t83s9zm3t0tvH55+fr6ZzTezrWa2wsy+nXPf4el9783Z1qv2Uev0PqPTj4ea2a6ar5nzHO3Szz9rZi+ZWe/086vN7BUz22ZmS8zsU034Xq81s7Xpcy00s9NrjfH5nM/bmtlqM1uZs22gmf3JzNab2VIz+0LObY117Ghm/2Fmy83sLTP7tZl1Tm+bmvt10m3PmtlVeX5fXcxsmpmVp2cN7DWzm+tpdLOZfa+uz9PX7v70+9uUfjw4575lZnZT2mWTmf053b4552yF3LMXLk9vf1/6mm42s6fM7Ih0+y9z7utmtiP9+KH09qfM7ON1fA/fa+D7m2pm1enzbDOzWWZ2VM7tp5jZc+lYVqRtL80ZR1X6fWw3s+05r+1dZvbH9Dnnmdmxee4X+30PuZ+b2Sgz+6uZbTSzDWZ2u5n1zPN5v502+0DOts+m22qev42ZfcvM3jSzdWZ2q5n1aIYWufv5/1rOn+86Xo/a3/9ZZrYs5/Mj0vtsTveR9+XcdnO6L2+3ZN++wd7+e2K/cdT1/GnfcjObkNNzg5lNrWesQ8zs7rT3RjP7Zc5tufvV9vTjmj9/PdK269PW3zKzNultuWf0bE1f70H1fP3aZ//UfJ2p6e0dzez/WfLnb3X6cccGnsvN7Es5285Lt+X++b/AzBak/Z8zs2NybltmZt8ws5ct+fN+k5l1yuexdYzHzewLlvwdvsHMfprTqCX20ylmNj19zBpL/q7pUM/YGvs3aKCZ3ZfuS4vM7BM5j23s74Zllvx7l3tG11Ppbf9jZv9Zayx/MbMvph/XuT9aI39viEhCE2OR1qkTsCefO5pZL+ALwOYDeP4dwBVAT+B84DNm9v5a9xlpZgPSjz8KLGng+f4d2FjP+C4DvgKc4+4191kHXAB0B64G/svS/+g2pJ7v9S/A2PS5/hf4Wa2HdTCzyenH5+c+Nv1P3F+A54FBwJnAF83snMbGkvoxMAYYD4xOn+Nf83zsPvV8X1eQfF8j0rMGftLAU1RT/78XbYCbgGHAUGAX8Muc228DugDjgH7AfwG4e8+csxX2nb3g7reb2RjgDuCLQF/gQeAvZtbB3f8x50wHgGPTz9+TR4qGrE6fsyfJ6/VtSH4oAzwE/CIdy3hggbv/MWcczwC1xwVwIfB/QBnwB+DPZtY+j/2iod4G/BAYCBwBDMkZaz7726tA7g8OrgLeqPX5VcAZwEigK+nr2cQWpM9xGHDQr5WZtU+/x0dJ9qfPA7eb2dicu/0k/dpHkvyZPDff53f3xcDX0ufsQrJv3+zuT9UxlrbA/cCbwHCS5nfm3KUNsCqnxfKc234B9CBpfDrJn8erc26fnj6mH8nf1V+ifrl/froCq3Nu+yZwAslrdSwwBfhWA8+1CLgy5/OPA6/kfM8TgBuBTwG9gd8A99WabF8OnAOMIvn761sH8NjaPgBMAiaQ/Hn6WLr9Kpp/P60i6dwHOJHkz89nGxhbTZO6/g26A1hJ8uf0YuAHZnZmzsPq/Lsh5/b35oztH3O23wJ8KOcHBH3Scd7RyP5Y798bIvI2TYxFWqc+1DPRrMM3Sf4zsyXfJ3f3p9z9BXevdveFJP9JOL3W3W4h+Y8NJP8pvKWu50qPKJxYz+3nAr8D3uPu+46euvsD7r7YE38j+U/0qXkM/R3fq7svcfeazw2YV+sxv+PticbH089rTAb6uvt33X2vuy8Bfgtc1thAzMyATwBfcvdyd98G/CCfx9ahrtfQ0l9t83j8cuDU3CM/Ndx9o7v/yd13pmP8Pulrnf7g4z3Ap919k7tXpK9HYy4FHnD3x9y9AvgPoDNwUh6Pbao2JE1q/nxcDjzu7nek49/o7gvyfK657n5X+j38jOQHUifQ+H6xHHhXzdGoXO6+KO2yx93Xp89b82crn/1tLnComQ02s+OAt9h/InU58LN0v98OfAO4LB1LU1rU+CHJD7oO1gkkk6Afpd/jX0kmAx+q475tSfbxfP+uA8Ddf0vyw4KZwACSPz91mUIy0bjO3Xe4+253z71WQwdgb+0HpROYS4FvuPs2d18G/CfJDwhra5P+OqDvIcflwHfdfV26v3ynnq9T4y1gmZmdaGb9SH7gNSvn9k8Av3H3me5e5e63kEzcT8i5zy/dfYW7l5P8ffChA3hsbT9O//5bDvy/nOdq9v3U3ee6+wx3r0xfk9/wzn+3anvHv0FmNgQ4Bfhauk8sAG5g/+71/d3Q2Bhnkfw9XjPJvgx4yt3fooH9sZG/N0QkpYmxSOs0guSnyg1Kf/J+CfDTOm6+JD3lbDOwodbjjjezJ9PTubaQHBXsU+vxtwGXm9nxwAqS/5DV5cfAvwAVddx2A7CMWv/Am9l7zGxGehrbZuC8Or4+tR5T7/dqZl8HdpL8h/7+WjffD0y15LTQASQTjxrDgIE1ndKx/DNwaM596uvYl+RI69yc2x9Ot9eo/dzv+I9VA9/XLcBsoOY1+krtx+b4H2A38Fb6dT6c8/xdzOw3lpzSuBV4GuiZ/ud/CFDu7psaeO66DCRn/3T3apJ9pM7TSevw85zTIW+sa0Jf19dMv7dtJJP5X6TbhwCL8x75/lbUfJB+DzVHkBrbL75P8me0Zv89peZ5zKyfmd1pZqvS3r/n7X07n/0N4GaSo5OfIPkzlGu/9unH7dLnaEoL0j/rh1PPD8Fq+XnO9/DnWuNbkfbMHWPuvvGV9HErgOkk+3mNmj9vG8zsMTMbWc/X/y1wFPALd6/v7JohwJvuXlnP7WVAXft+H5JJc+3Oud/DCen3sJlkX7i5nq/RmLpez4GNPOYGkh/yXQXcWuu2YcA/1drHhtR6zhU5H+d+vXweW1t9z9Xs+6mZjbFkKcja9M/WD2jk3w3q/jdoIMnfe9tqjS/39a3v74Z83AJ8JP34IyT/lkID+2Mjf2+ISEoTY5HWaRIwP4/7fY/ktMRtddw2zZPTYXvyzn9g/wDcBwxx9x7Ar0mO3OTaCLxI8lP52v85r/Gu9Lmn1XP7h0iOvHw//Sk96Wl5fyI5ynhoOr4H6/j6tdX7vbr7j0gmqVcB02qtzaoE7gHu4p3/eV0BLK3plP7q5u7n5dynvo4bSE5LHpfz2B6+/+mpq3OfG5iR7/fl7jtJJvUvkpzW+B91PLbmvuvd/d3p1+9J8vrW+CeSU7KPd/fuwGnpdku//7KDWMu2muQ/0ckTJUfPhwCr8nz8F9JxHgVM5O0zExr8muljOgNfJ9mHIPkeRuX5dWsbUvNBeurjYJLvrcH9wt3fcPfj3b17Oqbco5A/BBw4Ju39Ed7et/PZ3yD5T/GHSU5DfaB2B3Lak5weX0nyg6umtIDkdP2vu3u91zfI8YWc/fr9tcY3pOZU0pwx5u4b/5E+rhvJBPS6nNumpbcNJDky/4PaX9jMupIcnfwd8G0zK6tnjCuAoXUd2U+NAV6vY/sGkh/01e6c+z3MSMfZieT1urmer9GYul7P1fXct8ZDwMkkp1TfVuu2FcD3a+1jXdz9jpz7DMn5OPfr5fPY2up7rpbYT39FstTgsPTP1j/T+L8b7/g3KB1bmZl1qzW+3Ne3vr8b8vF74EJL1iUfwds/OGpof2zo7w0RSWliLNLKmNmlJP+heLyRu44GjieZuB6obiQ/Md9tZlPIOcJYy38BC0iOhNbl2ySnhXk9tz/j7i8CP88ZZwegI7AeqDSz9wBnNzLeer9XMzsy5z8anUnWf+6udbfrSdbh3V5r+yxgq5l9zcw6W3JxrqPs7TXJ9UqPIvyWZH10v3Qsgyz/9cmNfV89SLp9ooEjXvnoRjKB35xOIP6t5gZ3X0Pyn+z/teQiXe3N7LR6nifXNOB8MzszXXf3TySnXD53gGPbkT4u73/r0n2tmrd/SHE7cJaZXWLJxdF6m9n4PJ9uopldlO4/X0zHMoMm7BckvbeT9B7E/pO+vJ7X3TeTrJ39zzpe+zuAL5nZiHSC+APgj+n9mtLiXcmX9tpnXByomSSv61fT/Wkq8F72X9tbo4pkMtC39g3uvpekY137xn+TnOr6cZIfHPy6nrHMAtYAPzKzQ8ysk5mdDMnfGyTrYf9cx9euItnHv29m3cxsGPBlkgnPO+6efh/v+B7ydAfwLTPra8l61H+t5+vUHt+Pgd+np0Pn+i3waUvOCrL0+z6/1iTwc5acql9GMrn84wE8trbr0r87hgDX5jxXS+yn3YCtwHYzOxz4TB6Pece/Qe6+guTvqh+m+8QxwDXs/+9DfX83NCo9ZXs2yQ8t/uTuu9Kb6t0fafjvDRFJaWIs0opYcsXfO0mOQrxpb1+R89fAiWb2Us7dDwW+la6BOlCfBb5rZttI/iNW5xFfT9aaXdXAEaT5XsdFb+rwQ2CAmV2ZHhn9Qvo1N5FMyu9r5PENfa+fJ7mY1xaStYaXuPt+E2NP1rl9KJ1w5G6vIvlP+3hgKcmRohtILrqTj6+RXAxnRnr62+MkR2fz1dD39VPg7nTNWlP8P5IfGGwg+Y9d7R9yfJTk6NirJB2/2NgTuvtrJEc0fpE+73tJLkbzjvWa9fiJJVfsXkLS76Y8HjMw/fOwjeQ/8x9Lx7Kc5FT8fwLKSX6Qc2x9T1LLvSRHkzaRdLjIkzWPTdkvvkNyIaItJJO2u2tuOJDndfefuHtdZ2rcSPIf7qfT59hN8megqS0GAF/N8771SveB95Gc7r6B5IJ4V7j7qzl3+2r699pakv/n/Djntg+Y2UozW0XScb8LUZnZhSTrRmvewu7LwIT0787aY6npPZrk6PNK4FIzO4Tkuga/cff6znb5PMkEfwnJGQF/IGlf48T0e9gCXMT+F2A6EN8D5gALgRdIrpHwvQYfAbj7Te7+wzq2zyE5Bf+XJPv1It55RsYfSL7/Jemv7x3AY2u7l2R5ygKS/b3mGg4tsZ9+heTfi20kk/g/Nnz3/ez7Nyj9/EMkF8BaTXJG0b+5+2O1vq93/N1wAF/vFuBoco7o17c/pjfX+/eGiLzN6j8QIyKlxpK38pnq7lfVcdtwkot4DI8dlUjpseQtyka7+0cau69IqbDkba8+7u6NnZGUz3M5yWnNi5o8sCLSHH83pGfe/B4Y7vuvtxeRJtARYxERERGRDEiXl1wL3KBJsUjz0sRYpHW5jeRUtrq8yYGdpisiIiJBzOwIkiuVDyBZxiIizUinUouIiIiIiEirpiPGIiIiIiIi0qppYiwiIiIiIiKtWn1vSt/q9OnTx4cPH17oYdSr5pR3M70fe0tS5xjqHEetY6hzDHWOoc4x1DmGOsfJQuu5c+ducPc63xteE+PU8OHDmTNnTqGHISIiIiIiIi3AzN6s7zadSp0RCxcuZOHChYUeRslT5xjqHEetY6hzDHWOoc4x1DmGOsfJemsdMc6IefPmAXDMMccUeCSlTZ1jqHMctY6hzjHUOYY6x1DnGOocJ+ut9XZNqUmTJnkxn0pdVVUFQNu2bQs8ktKmzjHUOY5ax1DnGOocQ51jqHMMdY6ThdZmNtfdJ9V1m44YZ0Qx72ClRJ1jqHMctY6hzjHUOYY6x1DnGOocJ+uttcY4IxYsWMCCBQsKPYySp84x1DmOWsdQ5xjqHEOdY6hzDHWOk/XWmhhnRNZ3tKxQ5xjqHEetY6hzDHWOoc4x1DmGOsfJemutMU4V+xpjEREREREROXgNrTHWEWMRERERERFp1TQxzoi5c+cyd+7cQg+j5KlzDHWOo9Yx1DmGOsdQ5xjqHEOd42S9tSbGGfHSSy/x0ksvFXoYJU+dY6hzHLWOoc4x1DmGOsdQ5xjqHCfrrbXGOKU1xiIiIiIiIqVLa4xFRERERESkRWzbXcEzb6xn1eZdhR7KQWtX6AFIfmbPng3A5MmTCzyS0qbOMdQ5jlrHUOcY6hxDnWOocwx1bn7V1c6SDTuYt3wT85dvYt6bm3l93TbGtlnHuUf154sfek+hh3hQNDHOiNdffx3QH+qWps4x1DmOWsdQ5xjqHEOdY6hzDHVuum27K1iwYjPz3tzM/BWbmL98M1t2VQDQvVM7jhvai/OOHkCbJevoXrGhwKM9eFpjnNIaYxERERERac3qOxrsDmZwWL+uTBjaK/k1rCcj+3SlTRsr9LDz1tAaYx0xFhERERERaYW27a7g+RVbmLd8UzoZ3v9o8PihvXjP0f2ZMLQX44f2pHun9gUeccvRxDgjZsyYAcAJJ5xQ4JGUNnWOoc5x1DqGOsdQ5xjqHEOdY6jz29ydxevfPho8f/lmXntr/6PB7zmq/0EfDc56a02MM2Lp0qVAdne0rFDnGOocR61jqHMMdY6hzjHUOUZr7tzQ0eBu6drgc49qvqPBWW+tNcYprTEWEREREZEsck/XBr+5iXnLNzN/+aZ9R4MhZ23wsJ5MGNqLUX2ztTa4uWiNsYiIiIiISIk4kKPBxw7pSY/Opbs2uLloYpwRzz33HAAnnXRSgUdS2tQ5hjrHUesY6hxDnWOocwx1jlEKnfM5GnzuuP4FPxqc9daaGGfEypUrCz2EVkGdY6hzHLWOoc4x1DmGOsdQ5xhZ7Lx9TyXPr9icToQ3MX/FZjbvfPto8PghPTlnXH8mDOvF+CI6GpzF1rm0xjilNcYiIiIiIhKp5mjw/OWbk9Oi39zE629to1prg1uE1hiLiIiIiIgUWFaPBrcGmhhnxLPPPgvAKaecUuCRlDZ1jqHOcdQ6hjrHUOcY6hxDnWMUsrO7s3TDDuY1cDT47CMPTY8I92J0xo8GZ32f1sQ4I9auXVvoIbQK6hxDneOodQx1jqHOMdQ5hjrHiOycezR4/orkIlmbao4Gd2zH+KGlfTQ46/u01hintMZYRERERETy0djR4NH9ujJhaM+SORpcKrTGWERERERE5CDtqDkavPztt0yqfTT47HH9mTC0J8cN6UWPLqV1NLg10MQ4I/72t78BcPrppxd4JKVNnWOocxy1jqHOMdQ5hjrHUOcYB9O55mjwvitFL9/Ma2u37nc0+N0ltDa4uWR9n9bEOCM2btxY6CG0CuocQ53jqHUMdY6hzjHUOYY6x8incz5Hg9/9rsN0NLgRWd+ntcY4pTXGIiIiIiKlzd1ZtnHnvrdLqn00eFTfQ/YdCZ4wtBej+3WlrY4GlwytMRYRERERkVZn595KFqzYnJwWnV4tunzHXuCdR4PHD+lJzy4dCjxiKZQWmxib2Y3ABcA6dz+q1m1fAX4K9HX3Dem2bwDXAFXAF9z9kXT7ROBmoDPwIHCtu7uZdQRuBSYCG4FL3X1Z+pgrgW+lX+577n5LS32fUZ588kkAzjjjjAKPpLSpcwx1jqPWMdQ5hjrHUOcY6twyNu3Yy+xl5cxeVs6sZZtot/Zlqt2ZXzmIUX0P4czD++locAvJ+j7dkkeMbwZ+STJ53cfMhgDvBpbnbDsSuAwYBwwEHjezMe5eBfwK+CQwg2RifC7wEMkkepO7jzazy4AfA5eaWRnwb8AkwIG5Znafu29qwe+1xW3durXQQ2gV1DmGOsdR6xjqHEOdY6hzDHVuHis37UwmwUs3MWdZOW+s2w5Ah7ZtOHZID47t34E+XTty0wffraPBLSzr+3SLrjE2s+HA/blHjM3sLuDfgXuBSe6+IT1ajLv/ML3PI8C3gWXAk+5+eLr9Q8BUd/9UzX3cfbqZtQPWAn1JJthT3f1T6WN+Azzl7nc0NFatMRYRERERKV7V1c4b67Yza1k5c5aVM3tpOau37AaS06InDu/F5OFlTBlRxtGDetCpfdsCj1iKTdGsMTaz9wGr3P15s/1OWxhEckS4xsp0W0X6ce3tNY9ZAeDulWa2Beidu72Ox9QezydJjkYzdOjQg/umRERERESk2e2trOaFVVuYnU6E57y5ic3p1aL7devI5BFlfGp4GZOG9+Lw/t11WrQ0SdjE2My6AN8Ezq7r5jq2eQPbD/Yx+290vx64HpIjxnXdp1g8/vjjAJx11lkFHklpU+cY6hxHrWOocwx1jqHOMdT5nbbvqWTem5v2rRFesGIzuyuqARjZ5xDOObI/k4b3YsqIMoaWdaHWgbY6qXOcrLeOPGI8ChgB1BwtHgzMM7MpJEd1h+TcdzCwOt0+uI7t5DxmZXoqdQ+gPN0+tdZjnmrebyXerl27Cj2EVkGdY6hzHLWOoc4x1DmGOsdQZ9iwfQ+zl5Yze1kyGX55zVaqqp02BuMG9uBDU4YyZXgZk4aX0bdbx4P6GuocJ+utw9cY59y2jLfXGI8D/gBMIbn41hPAYe5eZWazgc8DM0kuvvULd3/QzD4HHO3un04vvnWRu1+SXnxrLjAh/VLzgInuXt7QWLXGWERERESkZbg7y8t3JpPgpckR4SUbdgDQsV0bxg/pyZQRZUweXsaEYb3o2lHvKivNryBrjM3sDpIjt33MbCXwb+7+u7ru6+4vmdk04GWgEvhcekVqgM/w9ts1PZT+AvgdcJuZLSI5UnxZ+lzlZvbvwOz0ft9tbFIsIiIiIiLNp6raeW3ttvRtk5ILZa3btgeAHp3bM2lYLy6ZPITJw5MLZXVo16bAI5bWrkWPGGdJsR8xfvTRRwE4++y6lmhLc1HnGOocR61jqHMMdY6hzjFKrfOeyioWrtzCrPRo8Nw3N7FtdyUAA3p0YvLwMiaPKGPK8DIO69eVNkEXyiq1zsUsC62L5qrUcvAqKioKPYRWQZ1jqHMctY6hzjHUOYY6x8h65627K5j75tunRT+/cgt7K5MLZY3u15ULjhnIlBHJ2ycN6tk5rwtltYSsd86SrLfWEeNUsR8xFhEREREplHVbd+87JXrWsk28unYr7tCujTFuUA+mpO8hPGl4GWWHdCj0cEXqpCPGIiIiIiKSF3dn6YYdyfrgpckVo5eX7wSgc/u2TBjWk2vPPIzJw8s4bmhPunTQlEKyT3txRjz88MMAnHvuuQUeSWlT5xjqHEetY6hzDHWOoc4xiqlzZVU1r6zZtu+I8Jw3y9mwfS8AZYd0YNKwXnz0hGFMHlHGuIHdad82OxfKKqbOpS7rrTUxFhERERFpRXZXVDF/+WZmL0vWB897cxM79iZvCDO4V2dOPawvk4eXMWVEL0b17Vqw9cEikbTGOKU1xiIiIiJSijbv3MucZZuY/WZyRPiFVVuoqErmAGMP7cbk9CJZU0aUMaBH5wKPVqTlaI2xiIiIiEgrsXrzrn1Hg2cv3cRrb20DoH1b4+hBPfjYKSOYMryMScPK6NGlfYFHK1IcNDHOiAceeACA888/v8AjKW3qHEOd46h1DHWOoc4x1DlGc3V2dxat286sZeXMWbaJWUvLWbV5FwCHdGjLhGG9uOCYAUweUcb4IT3p1L5tk8eeJdqf42S9tSbGGdG+vX6aF0GdY6hzHLWOoc4x1DmGOsc42M4VVdW8uGpLMgleVs6cZeVs2pm8f2yfrh2YPLyMa04ZwZQRZRzevxvtMnShrJag/TlO1ltrjXFKa4xFREREpNjs3FvJ/OWbmbU0OTV6/vLN7KpILpQ1rHeXZG3w8DImjyhjeO8uulCWSAO0xlhEREREJAPKd+xN1wYnE+EXV2+lqtoxgyP6d+fSyUOYPLyMycN70a97p0IPV6RkaGKcEX/5y18AeO9731vgkZQ2dY6hznHUOoY6x1DnGOoco6bz+JPP3HehrFlLy1m8fgcAHdq1Yfzgnnz69JFMGl7GxGG96N4p26eqFoL25zhZb62JcUZ07qxL50dQ5xjqHEetY6hzDHWOoc4tx91ZumEHM5eW8/yyrazZvJvP//1JALp1asekYb24aMJgpowo4+hBPVrdhbJagvbnOFlvrTXGKa0xFhEREZHm5O4s2bCDGUs2MmNJOTOXbGTdtj0A9OnakeNHpuuDh5cxtn832rbR+mCRlqQ1xiIiIiIiLczdWbx+O9PTSfCMJeVs2J5MhPt168jxI3tzwsgyThjZm5F9DtGFskSKiCbGGXHvvfcCcOGFFxZ4JKVNnWOocxy1jqHOMdQ5hjrnz915Y912ZizZyMwl5cxcupEN2/cCcGj3jpw8ujcnjOzN8SPKGFFrIqzOMdQ5TtZba2KcEd27dy/0EFoFdY6hznHUOoY6x1DnGOpcv+pq5/V125i5pDyZDC8tp3xHMhEe0KMTpx7Wl+NHJEeEhzXy1knqHEOd42S9tdYYp7TGWERERERyVVc7r67dxsylG5mxZCOzlpazaWcFAIN6dub4kWWcMCI5KjykrLNOjRYpclpjLCIiIiLSiOpq55W1W5mRHhGevayczelEeHCvzrzr8EP3rREeUtalwKMVkeakiXFG3H333QBcdNFFBR5JaVPnGOocR61jqHMMdY7RmjpXVTuvrNm676rRs5ZuZOvuSgCGlnXh3UccmqwRHlnG4F7NOxFuTZ0LSZ3jZL21JsYZ0bt370IPoVVQ5xjqHEetY6hzDHWOUcqdK6uqeTmdCM9cUs6sZeVsSyfCw3t34T1HDeCEUWUcP6I3A3u27HuylnLnYqLOcbLeWmuMU1pjLCIiIlJaKquqeXH11vStkzYye9kmtu9JJsIj+hzCCSOTSfDxI8sY0KNlJ8IiUnhaYywiIiIiJa+iqpoXVm3Zd9XoOcvK2bG3CoCRfQ/hfeMH7rtq9KHdOxV4tCJSTDQxzoi77roLgIsvvrjAIylt6hxDneOodQx1jqHOMbLUuaKqmoUrt+x766Q5y8rZmU6ER/fryvuPG7TvfYT7FdlEOEuds0yd42S9tSbGGdG/f/9CD6FVUOcY6hxHrWOocwx1jlHMnfdWVrNw5eacifAmdlUkE+Exh3blHyYM5oSRvZkyooy+3ToWeLQNK+bOpUSd42S9tdYYp7TGWERERKS47Kms4vkVNUeENzL3zU3srqgGYOyh3fa9ddKUEWX07lrcE2ERKTytMRYRERGRore7oooFKzbvWyM8b/km9lQmE+HD+3fjsslDOWFkGVNG9KbskA4FHq2IlBJNjDNi2rRpAFxyySUFHklpU+cY6hxHrWOocwx1jhHZeXdFFfOWb9o3EZ6/YjN7K6sxgyP6d+fy44dx/Mgypgwvo1eJTYS1P8dQ5zhZb62JcUYMHjy40ENoFdQ5hjrHUesY6hxDnWO0ZOdde6uYv3wTM5ZsZMbSchYs38zeqmQiPG5gdz56wrDk1OjhZfTo0r7FxlEMtD/HUOc4WW+tNcYprTEWERERaV4791Yy783N+9YIL1ixmYoqp43BuIE99q0RnjS8jB6dS3siLCKFV5A1xmZ2I3ABsM7dj0q3/RR4L7AXWAxc7e6b09u+AVwDVAFfcPdH0u0TgZuBzsCDwLXu7mbWEbgVmAhsBC5192XpY64EvpUO5XvufktLfZ8iIiIiktixp5K5b25i5tKNzFhSzsKVb0+Ejx7Ug4+dPILjR5YxaXgZ3TtpIiwixaMlT6W+GfglyeS1xmPAN9y90sx+DHwD+JqZHQlcBowDBgKPm9kYd68CfgV8EphBMjE+F3iIZBK9yd1Hm9llwI+BS82sDPg3YBLgwFwzu8/dN7Xg99ri7rjjDgA+9KEPFXgkpU2dY6hzHLWOoc4x1DnGgXTevqeSOcvKmbk0WSP8wsotVFY7bdsYRw/qwTWnjEwmwsN60U0T4f1of46hznGy3rrFJsbu/rSZDa+17dGcT2cANe/+fCFwp7vvAZaa2SJgipktA7q7+3QAM7sVeD/JxPhC4Nvp4+8CfmlmBpwDPObu5eljHiOZTN/RzN9iqBEjRhR6CK2COsdQ5zhqHUOdY6hzjIY6b9tdwZw30zXCS8p5cdUWqqqddm2MYwb34BOnjeSEkb2ZOKwXXTvqUjYN0f4cQ53jZL11If/G+hjwx/TjQSQT5Ror020V6ce1t9c8ZgVAegR6C9A7d3sdj8msE044odBDaBXUOYY6x1HrGOocQ51j5HbeuruCOcvKmbGknJlLNvLCqi1UO7Rvaxw7uCefPv3tiXCXDpoIHwjtzzHUOU7WWxfkbzAz+yZQCdxes6mOu3kD2w/2MbXH8UmS07QZOnRoAyMWERERKX3bdlcwKz0tesaScl5a/fZEePyQnnzujNEcP6I3E4b11ERYREpKo3+jmdlZ7v54rW1XHuwFrdILY10AnOlvXxJ7JTAk526DgdXp9sF1bM99zEozawf0AMrT7VNrPeapusbi7tcD10NyVeqD+X6i3H578jOEyy+/vMAjKW3qHEOd46h1DHWOoc7Nb3dFFfPe3MRzizfy98UbWLhyC+9q9xptzOg88Hj+8V2HccKIMo4b2ovOHdoWerglRftzDHWOk/XW+fyo71/N7B+ArwBdgRuAPcABT4zN7Fzga8Dp7r4z56b7gD+Y2c9ILr51GDDL3avMbJuZnQDMBK4AfpHzmCuB6SRrlf+aXq36EeAHZtYrvd/ZJBf5yrQxY8YUegitgjrHUOc4ah1DnWOoc9NVVlWzcNUWnlu0gecWb2TOm5vYW1lN23SN8KdPH8nQqs4MKevCSSccX+jhljTtzzHUOU7WWzf6PsbpBa3+CfhUuulf3b3RC1mZ2R0kR277AG+RXCn6G0BHkrdXApjh7p9O7/9NknXHlcAX3f2hdPsk3n67poeAz6cT4E7AbcBxJEeKL3P3JeljPgb8c/o1vu/uNzU2Xr2PsYiIiJSa6mrn1bXbeG7xBqYv3sjMpeVs31MJwOH9u3Hy6D6cNKo3U0aU6arRIlLyGnof43wmxmXAb4BuJKcl/x74sTf2wIzRxFhERESyzt1ZtnEnzy3ewHOLNjJ9yUbKd+wFYESfQzhxVG9OGtWbE0f2pnfXjgUerYhIrIYmxvmcSj0D+JG732hmnUneL/jvwEnNOEZpxK23Jm8HfcUVVxR4JKVNnWOocxy1jqHOMdS5bmu37Obv6anR0xdvYPWW3QAc2r0jU8f05aTRfThxVG8G9eyc1/Opcwx1jqHOcbLeOp+J8VnuvhzA3XcBXzCz01p2WFLbuHHjCj2EVkGdY6hzHLWOoc4x1Dmxacdepi/ZuO+o8JINOwDo1aU9J47qzWdGJadHj+xzCMmKuAOjzjHUOYY6x8l660ZPpQZIL2R1GNCpZpu7P92C4wqnU6lFRESkGG3fU8nspeX7jgq/snYr7nBIh7ZMGVHGSaP6cNLo3hzRvztt2hz4RFhEpLVo0qnUZvZx4FqS9cULgBNIrgT9rmYco4iIiIiQvIXS/OWbkyPCizfy/IrNVFY7Hdq2YcKwnnz5rDGcNLo3xwzuSfu2bQo9XBGRkpDPqdTXApNJriB9hpkdDnynZYcltd18880AXHXVVQUdR6lT5xjqHEetY6hzjFLtXFlVzQurtqRrhDcye1k5eyqraWNwzOCefPK0kZw0qg+ThveiU/uWfy/hUu1cbNQ5hjrHyXrrfCbGu919t5lhZh3d/VUzG9viI5P9jB8/vtBDaBXUOYY6x1HrGOoco1Q6uzuvvbWN5xYl64RnLilnW85bKH34+KGcPKoPU0aW0b0Ab6FUKp2LnTrHUOc4WW+dz9s13QNcDXyR5PTpTUB7dz+vxUcXSGuMRUREpCW4O8vLd/L3dCI8ffFGNqZvoTSsd5dkjfCo3pw4qjd99BZKIiItpklrjN39A+mH3zazJ4EewMPNOD7JQ1VVFQBt27b8KVStmTrHUOc4ah1DnWNkqfNbW3fvu2r0c4s3smrzLgD6devIaWP67ns/4cG9uhR4pO+Upc5Zps4x1DlO1lvnc/Gt6939kwDu/reWH5LU5bbbbgOye85+VqhzDHWOo9Yx1DlGMXfevHMvM5Zs3HdUePH65C2UenRuz4kje/Pp00dy4qg+jOp7cG+hFKmYO5cSdY6hznGy3jqfNcZ1HmqWWBMmTCj0EFoFdY6hznHUOoY6xyimzjv2VDJ7WTnPLU4mwi+tTt5CqUv6FkqXTh7CSaP6cOSA7L2FUjF1LmXqHEOd42S9dT5rjNcBd9be7u5faKlBFYLWGIuIiEh99lTWvIXSRqYv3sD85W+/hdJxQ3ty8uhknfAxg3vSoZ3eQklEpBg1aY0xsAuY27xDkgNVUVEBQPv28VenbE3UOYY6x1HrGOocI7JzVbXzYvoWSs8t3sDsZeXsrkjeQunoQT34xGkjOWlUbyYNK6Nzh2yup6uP9ucY6hxDneNkvXU+E+Nyd7+lxUciDbr99tuB7J6znxXqHEOd46h1DHWO0ZKd3Z031m3n74s28NzijcxYspFtu5O3UBp7aDcumzyUk0b15viRvenROZv/6cuX9ucY6hxDneNkvXU+E2NNiovApEla6h1BnWOocxy1jqHOMZq784rynfsmws8t3siG7XsAGFrWhfOPHsBJo/tw4sje9O3Wut5CSftzDHWOoc5xst46nzXGda6idvd5LTKiAtEaYxERkdK2butupi/ZuG8yvHJT8hZKfbt15ORRvTlpVB9OHNWbIWXF9xZKIiLSdE1dYzwHeANYBdRcVtGBdzXP8CQfu3fvBqBTp04FHklpU+cY6hxHrWOoc4wD7bxlZwUzlm7kuXQi/Ma67QB079SOE0f15hOnjuTk0b0Z1bdr0b+FUiTtzzHUOYY6x8l663wmxu8G/pXkAlw/dPfylh2S1OXOO5MLg2f1nP2sUOcY6hxHrWOoc4zGOu/cW8nsZZt4bvEGnlu0kRdXb8EdOrdvy+QRZVw8cXDyFkoDu9M2Y2+hFEn7cwx1jqHOcbLeutGJsbs/ATxhZhcBD5jZ/cB/ufvOFh+d7HP88ccXegitgjrHUOc4ah1DnWPU7ry3spoFKzbvmwjPX7GJiiqnfVvjuKG9uPbMwzh5dB+O1VsoHRDtzzHUOYY6x8l663zWGH8559N2wEeAfu7evyUHFk1rjEVERIpbVbXz8uqt/H1xcmr07KXl7KqowtK3UDpxVG9OHtWHScN70aVDPifFiYhIa9LUNcbdan3+p6YPSQ7Uzp3JAfouXXRBkJakzjHUOY5ax1DnlrNh+x6efn09T722npmvr2bzrgr20J7D+nXl0slDOHFUb04Y0ZseXUr7LZQiaX+Ooc4x1DlO1lvncyr1dyIGIg2bNm0akN1z9rNCnWOocxy1jqHOzaeq2nl+5Waeem09T722jhdWJeuE+3TtwHu6LKVH7/ZcceWV9OuWzYu7ZIH25xjqHEOd42S9daMTYzO7FPgg8Cvgf4Ay4Mvu/vsWHpvkOPHEEws9hFZBnWOocxy1jqHOTZN7VPiZN9azaWcFbQyOG9qLL581hqlj+zFuYHfeeGMogCbFLUz7cwx1jqHOcbLeOp81xq8D/wzcAEwCtgNPuPu4lh9eHK0xFhERidHQUeHTxvTljLH9OPWwPvTs0qHQQxURkRLS1DXGO9z9LjP7F3dflD7hnmYdoTRq+/bkvRe7du1a4JGUNnWOoc5x1DqGOjcu36PCbRp4GyV1jqHOMdQ5hjrHyXrrfCbGg8zs58CA9HcDBrXssKS2u+66C8juOftZoc4x1DmOWsdQ53dq6KjwGYf3O6ijwuocQ51jqHMMdY6T9db5TIyvS3+fm7NN5xwHO+WUUwo9hFZBnWOocxy1jqHOieY4KtwQdY6hzjHUOYY6x8l663zWGH/S3a8PGk/BaI2xiIjIgdFaYRERyZKmrjH+NFDyE+Nit2XLFgB69OhR4JGUNnWOoc5x1DpGa+rc0keFG9KaOheSOsdQ5xjqHCfrrfOZGPc0s4tqb3T3u1tgPFKPe+65B8juOftZoc4x1DmOWsco5c4tsVb4YJVy52KizjHUOYY6x8l663wmxj2AC0guulXDAU2MA5122mmFHkKroM4x1DmOWscotc6FPCrckFLrXKzUOYY6x1DnOFlvnc8a4/nuftwBP7HZjSQT6nXuflS6rQz4IzAcWAZc4u6b0tu+AVwDVAFfcPdH0u0TgZuBzsCDwLXu7mbWEbgVmAhsBC5192XpY64EvpUO5Xvufktj49UaYxERaa20VlhERFqDpq4xfukgv+7NwC9JJq81vg484e4/MrOvp59/zcyOBC4DxgEDgcfNbIy7VwG/Aj4JzCCZGJ8LPEQyid7k7qPN7DLgx8Cl6eT734BJJEe255rZfTUT8KzatCkZfq9evQo8ktKmzjHUOY5ax8hi52I9KtyQLHbOInWOoc4x1DlO1ls3OjF2948czBO7+9NmNrzW5guBqenHtwBPAV9Lt9/p7nuApWa2CJhiZsuA7u4+HcDMbgXeTzIxvhD4dvpcdwG/NDMDzgEec/fy9DGPkUym7ziY76NY3HvvvUB2z9nPCnWOoc5x1DpGFjo3tlZ46th+nFbkR4Wz0LkUqHMMdY6hznGy3jqfI8bN6VB3XwPg7mvMrF+6fRDJEeEaK9NtFenHtbfXPGZF+lyVZrYF6J27vY7HZNbUqVMLPYRWQZ1jqHMctY5RrJ2zeFS4IcXaudSocwx1jqHOcbLeOnpiXJ+6/kX2BrYf7GP2/6JmnyQ5TZuhQ4c2PsoCGj58eKGH0Cqocwx1jqPWMYqlcykcFW5IsXQudeocQ51jqHOcrLc+oImxmXUAOrr7toP8em+Z2YD0aPEAYF26fSUwJOd+g4HV6fbBdWzPfcxKM2tHcvXs8nT71FqPeaquwbj79aTv0Txp0qSGr0JWYBs2bACgT58+BR5JaVPnGOocR61jFLJzqR0Vboj25xjqHEOdY6hznKy3bnRibGZfAi4Hfg58F+hkZv/p7j89iK93H3Al8KP093tztv/BzH5GcvGtw4BZ7l5lZtvM7ARgJnAF8ItazzUduBj4a3q16keAH5hZzarvs4FvHMRYi8r9998PZPec/axQ5xjqHEetY0R2LvWjwg3R/hxDnWOocwx1jpP11vm8XdMikitG/5XkbZZ2A3Pc/chGHncHyZHbPsBbJFeK/jMwDRgKLAc+mHORrG8CHwMqgS+6+0Pp9km8/XZNDwGfTyfAnYDbgONIjhRf5u5L0sd8DPjndCjfd/ebGgtR7G/XtGJFsmx6yJAhjdxTmkKdY6hzHLWO0dKdGzoqPHVM35I6KtwQ7c8x1DmGOsdQ5zhZaN3Q2zXlMzGe5+4Tct/PuGZbC4y1YIp9YiwiIq1HY+8rXMpHhUVERFpKU9/HeKSZ3QeMSH83YERzDlAat25dshy7X79+jdxTmkKdY6hzHLWO0RydW9Na4YOl/TmGOsdQ5xjqHCfrrfOZGF+Y/v6fOdv+owXGIg148MEHgeyes58V6hxDneOodYyD6dya1wofLO3PMdQ5hjrHUOc4WW/d6KnUrUWxn0q9atUqAAYNyvxbMhc1dY6hznHUOka+nes7Kjx+SE/OGNtPR4Ubof05hjrHUOcY6hwnC62btMa4tSj2ibGIiGSP1gqLiIgUj6auMZYisHbtWgD69+9f4JGUNnWOoc5x1DpGbueGjgp/6awxnKGjwgdN+3MMdY6hzjHUOU7WWx/QxNjMOgAd3X1bC41H6vHwww8D2T1nPyvUOYY6x1Hrlufu3HXv/WzeWcGs9kdrrXAL0v4cQ51jqHMMdY6T9db5vF3Tl4DLgZ8D3wU6Af/p7j9t+eHFKfZTqbP+E5isUOcY6hxHrVuGuzN/xWYeWLiGh15Yw+6t5ZjBsMEDmTq2n44KtxDtzzHUOYY6x1DnOFlo3dT3MV4EXAb8FRgO7AbmuPuRzTzOgir2ibGIiBRWdXUyGX7whWQyvHrLbjq0bcOph/XhvKMHcOYR/XRUWEREpIg1dY3xVnefY2aL3b08fcLdzTpCaVQWrvJWCtQ5hjrHUeumSSbDm3hg4VoeenENa9LJ8Glj+vCVc8Zy1pGH0r1Te1atWsWOTevp2UWdW5L25xjqHEOdY6hznKy3zmdiPNLM7gNGpL8bMKJlhyW1PfbYY0B2z9nPCnWOoc5x1PrAVVc785Zv4oEX1vDQC2tZu7VmMtyXr547ljOPSCbDudQ5hjrHUOcY6hxDneNkvXU+p1KfXtd2d/9bi4yoQIr9VOp169YB0K9fvwKPpLSpcwx1jqPW+amuduYu38QDC9fw8IvpZLhdG04f05fz09Oku9WaDOdS5xjqHEOdY6hzDHWOk4XWTX4fYzM7FJicfjrL3dc14/iKQrFPjEVEpHnlToYfenENb23dc0CTYREREcmWJq0xNrNLgJ8CT5GcRv0LM7vO3e9q1lFKg1asWAHAkCFDCjyS0qbOMdQ5jlrvr7ramfPmpuQCWjmT4alj+nL+MQN41+EHNxlW5xjqHEOdY6hzDHWOk/XW+awx/iYwueYosZn1BR4HNDEO9MQTTwDZPWc/K9Q5hjrHUWuoqnbmLCtPJ8NrWbdtDx3btWHq2L7p1aQPpWvHfP45rJ86x1DnGOocQ51jqHOcrLfOZ43xC+5+dM7nbYDnc7eVgmI/lXrDhg0A9OnTp8AjKW3qHEOd47TW1lXVzuycyfD6dDJ8xth+nJceGW7qZDhXa+0cTZ1jqHMMdY6hznGy0Lqp72P8U+AY4I5006XAC+7+1WYdZYEV+8RYREQaVlXtzFqaTIYffuntyfC7Du/HeUcnk+FDmnEyLCIiItnSpDXG7n6dmV0EnEKyxvh6d7+nmccojVi2bBkAw4cPL+g4Sp06x1DnOKXeumYy/MALq3n4xbfYsH0PndqnR4YDJ8Ol3rlYqHMMdY6hzjHUOU7WW+dz8a0ykgtvPZW7zd3LW25YUttTTz0FZPec/axQ5xjqHKcUW1dVOzOXbkyODOdMhmuODJ8xNv7IcCl2LkbqHEOdY6hzDHWOk/XW+ZxKvQdYlbsJcHcf2ZIDi1bsp1Jv2rQJgF69ehV4JKVNnWOoc5xSaV1ZVZ0eGV7DIy+tZcP2vXRu3/btyfDhfenSoXCnSZdK52KnzjHUOYY6x1DnOFlo3dQ1xvPd/bgWGVkRKfaJsYhIa1NZVc3Mmsnwi2vZuOPtyfD5xwxg6tjCToZFREQkW5q0xhjoYWYXAnuA1cDL7l7ZnAOUxi1ZsgSAkSNL6kB90VHnGOocJ2utK6uqmbEkmQw/+lLOZPiIfpyfnibduUPbQg/zHbLWOavUOYY6x1DnGOocJ+ut85kY/w34B6AzMBAYZmafcPeHWnRksp+nn34ayO6OlhXqHEOd42Sh9duT4dU88tJblO/YS5cO6ZHhowcwtUgnw7my0LkUqHMMdY6hzjHUOU7WWzd6KvU7HmA2Gvizux/VMkMqjGI/lXrLli0A9OjRo8AjKW3qHEOd4xRr68qqaqYvSS6glTsZPvOIQzn/6P6cPqb4J8O5irVzqVHnGOocQ51jqHOcLLRu0hrjep5wgLuvafLIikixT4xFRLKuoqqa6YtrJsNr2bSzgkPSyfB5Rydrhju1z85kWERERLKlSWuMzewW4Fp335x+3gv4PvCx5hykNGzRokUAjB49usAjKW3qHEOd4xS6dUVVNc8t3siDC9fwyMtr2ZwzGT7/mAGcPqY0JsOF7txaqHMMdY6hzjHUOU7WW+ezxviYmkkxgLtvMrOSv0p1sXn22WeB7O5oWaHOMdQ5TiFa10yGH1i4mkdffmvfZPisI5Mjw6UyGc6lfTqGOsdQ5xjqHEOd42S9dT5v1/Q8MNXdN6WflwF/c/ejA8YXpthPpd6+fTsAXbt2LfBISps6x1DnOFGtK6qq+fuiDTz4wpp9k+GuHdtx1hHJ+wyfVoKT4Vzap2Oocwx1jqHOMdQ5ThZaN/Xtmv4TeM7M7gIcuITkVGoJVMw7WClR5xjqHKclW++trObvizfw4MJkMrxlVwXdOrbbd2T41MP6lPRkOJf26RjqHEOdY6hzDHWOk/XWjU6M3f1WM5sDvAsw4CJ3f7nFRyb7ee211wAYO3ZsgUdS2tQ5hjrHae7WeyuTI8M17zO8dXflvsnw+UcP4NQxfejYrnVMhnNpn46hzjHUOYY6x1DnOFlvnc8RY9KJsCbDBTR9+nQguztaVqhzDHWO0xyt91ZW8+yi9TywcC2Pvfz2ZPjdNUeGW+lkOJf26RjqHEOdY6hzDHWOk/XWB/V2TU3+omZfAj5Ocmr2C8DVQBfgj8BwYBlwSc665m8A1wBVwBfc/ZF0+0TgZqAz8CDJ1bPdzDoCtwITgY3Ape6+rKExFfsa4507dwLQpUuXAo+ktKlzDHWOc7CtaybD9y9cw2Mvv8W23ZV065RMhs8/egCnHKbJcC7t0zHUOYY6x1DnGOocJwutm/19jJs4mEHAs8CR7r7LzKaRTGqPBMrd/Udm9nWgl7t/zcyOBO4ApgADgceBMe5eZWazgGuBGelz/NzdHzKzz5JcTfvTZnYZ8AF3v7ShcRX7xFhEpKXtqazi2TeS06RzJ8NnH9mf84/pz8mjNRkWERGR7Grq+xgfWXtNsZlNdfenmjCmdkBnM6sgOVK8GvgGMDW9/RbgKeBrwIXAne6+B1hqZouAKWa2DOju7tPTMd0KvB94KH3Mt9Pnugv4pZmZF+LweDN55ZVXADjiiCMKPJLSps4x1DlOY633VFbxzOvJ1aQfe/kttu2ppHundpwzrj/nHz2Ak0f3oUO7NpFDziTt0zHUOYY6x1DnGOocJ+ut81ljPM3MbgN+AnRKf58EnHgwX9DdV5nZfwDLgV3Ao+7+qJkd6u5r0vusMbN+6UMGkRwRrrEy3VaRflx7e81jVqTPVWlmW4DewIaDGXMxmDlzJpDdHS0r1DmGOsepq/XuiiqeeSOZDD+eOxk+qj/nHzOAk0dpMnygtE/HUOcY6hxDnWOoc5yst87nfYwPAX5Msl63G3A78GN3rz6oL2jWC/gTcCmwGfg/0qO67t4z536b3L2Xmf0PMN3df59u/x3JadPLgR+6+1np9lOBr7r7e83sJeAcd1+Z3rYYmOLuG2uN5ZPAJwGGDh068c033zyYbynE7t27AejUqVOBR1La1DmGOsepaU3b9jz9+vpkMvzKOrbvqaRH5/acfeShnKfJcJNpn46hzjHUOYY6x1DnOFlo3dT3Ma4gObLbmeSI8dKDnRSnzkqfY306uLuBk4C3zGxAerR4ALAuvf9KYEjO4weTnHq9Mv249vbcx6w0s3ZAD6C89kDc/XrgekjWGDfhe2pxxbyDlRJ1jqHOMfZUVvH04s3vmAyfd3R/zjt6ACdpMtxstE/HUOcY6hxDnWOoc5yst85nYjwbuBeYTHI68m/M7GJ3v/ggv+Zy4AQz60Iy4T4TmAPsAK4EfpT+fm96//uAP5jZz0guvnUYMCu9+NY2MzsBmAlcAfwi5zFXAtOBi4G/Znl9McCLL74IwFFHHVXgkZQ2dY6hzi3H3Zm/YjN3z1vJ/QvX0GvPW3Tp0Jbzjz6K844ZwEmjetO+rSbDzU37dAx1jqHOMdQ5hjrHyXrrfCbG17h7zeWa1wIXmtlHD/YLuvtMM7sLmAdUAvNJjtp2JVnPfA3J5PmD6f1fSq9c/XJ6/8+5e1X6dJ/h7bdreij9BfA74Lb0Ql3lwGUHO95iUXPF7KzuaFmhzjHUufmtKN/JPfNXcc/8VSzdsIOO7dpw9rj+jNr0Jj06t+fqi48p9BBLmvbpGOocQ51jqHMMdY6T9db5rDEeWtd2d1/eIiMqkGJ/u6aKigoA2rdvX+CRlDZ1jqHOzWPr7goeXLiGu+etYtayZLXICSPLuOi4wbzn6P5069RerYOocwx1jqHOMdQ5hjrHyULrpq4xfgBwwGr9rsMPgYp5Bysl6hxDnQ9eRVU1T7++nrvnr+Kxl99ib2U1I/sewnXnjOXC8QMZ3KvLfvdX6xjqHEOdY6hzDHWOoc5xst660Ymxux8NYGZGcuGs9sCjLTwuqWXhwoUAHHOMfh7RktQ5hjofGHfnxVVbuXv+Su5bsJqNO/bSq0t7PjR5CBdNGMwxg3uQ/BX9TmodQ51jqHMMdY6hzjHUOU7WW+dzxLjGfwHHAluAjwAfbpERSZ3mzZsHZHdHywp1jqHO+Vm9eRd/XrCKe+at4o112+nQtg1nHdmPDxw3mNPH9M3ritJqHUOdY6hzDHWOoc4x1DlO1ls3usZ43x3NFgAT3L3azGa4+wktOrJgxb7GuKoqud5Y27ZtCzyS0qbOMdS5ftv3VPLwi2u5e95Kpi/ZiDtMGtaLiyYM5vyjB9Cjy4GdpqTWMdQ5hjrHUOcY6hxDneNkoXVT1xjXqM55/+K9TR+WHIhi3sFKiTrHUOf9VVU7zy7awD3zVvLIS2+xq6KKoWVduPbMw/jAcYMY1vuQg35utY6hzjHUOYY6x1DnGOocJ+utG50Ym9k2kottdTGzrSQX38r2uzdn0IIFCwAYP358QcdR6tQ5hjonXlmzlbvnreTeBatZt20P3Tu14wMTBvEPEwYxYWivetcNHwi1jqHOMdQ5hjrHUOcY6hwn663zufhWt4iBSMOyvqNlhTrHaM2d123dzb0LVnP3/FW8smYr7doYU8f24x8mDOKMw/vRqX3z/rS1NbeOpM4x1DmGOsdQ5xjqHCfrrfN5H+PT6tru7k+3yIgKpNjXGItIdu3aW8WjL6/lT/NW8ewb66l2OHZITy46bhDvPXYgZYd0KPQQRUREREpeU9cYX5f+fgrwbPqxAyU1MRYRaU7V1c6MpRu5e94qHnphDTv2VjGoZ2c+O3U07z9uEKP7dS30EEVEREQklc+p1O8FMLP5NR9LvLlz5wIwceLEAo+ktKlzjFLuvGjdNu6et4o/z1/F6i276dqxHecfM4CLJgxmyvAy2rRp+rrhA1HKrYuJOsdQ5xjqHEOdY6hznKy3PpCrUuf3vk7SIl566SUguztaVqhzjFLrvHH7Hu57fjX3zF/FwpVbaNvGOPWwPnz9vCN49xGH0rlD4a7SWGqti5U6x1DnGOocQ51jqHOcrLfOZ43xl9MPvwz8rGa7u/+s7kdkk9YYi8iB2F1RxROvrOPueSv52+vrqax2xg3szgeOG8T7xg+kXzddvF9ERESkmDR1jXHNVal/m/OxiEir4+7MeXMTd89byf0L17BtdyWHdu/INaeO4KLjBjO2v/6KFBEREcmifNYYfydiINKw2bNnAzB58uQCj6S0qXOMrHVetmEHd89fxT3zV7KifBddOrTl3HH9uWjCYE4c1Zu2weuGD0TWWmeVOsdQ5xjqHEOdY6hznKy3znuNsZmdB1wPtAW+4u63t9io5B1ef/11ILs7Wlaoc4wsdN68cy9/WbiGe+atZN7yzZjByaP68KWzxnDOuP4c0vFALtFQOFloXQrUOYY6x1DnGOocQ53jZL11o2uM993RbCZwObAJeMzdJ7TkwKJpjbGI7K2s5snX1nHPvFX89dV17K2qZsyhXblowmAuHD+QAT06F3qIIiIiInKQmrrGuEZ7d1+UPuH2ZhmZiEiBuTsLVmzm7nmr+MvC1WzeWUGfrh346InD+MBxgxg3sDtmxXuqtIiIiIg0XaMTYzP7efrh4PRjA0a26KjkHWbMmAHACSecUOCRlDZ1jlEMnVeU7+TP81dxz/xVLNmwg47t2nD2uP5cdNwgTj2sD+3atinY2JpTMbRuDdQ5hjrHUOcY6hxDneNkvXU+R4zn1vodQOccB1u6dCmQ3R0tK9Q5RqE6b91dwYML13D3/FXMWloOwPEjyvj06aM49+j+dO/UPnQ8EbRPx1DnGOocQ51jqHMMdY6T9dZ5rzEudVpjLFKaKqqqeeaN9fxp3ioef/kt9lRWM7LvIVx03CAuHD+IIWVdCj1EEREREQnQpDXGZrYNyJ09G+Du3r2Zxici0qzcnZdWb+VP81byl+dXs2H7Xnp1ac9lk4fwgQmDOXZwD60bFhEREZF98jmVepG7H9fiI5EGPffccwCcdNJJBR5JaVPnGC3Vec2WXfx5/mrunreSN9Ztp0PbNpx5RD8+cNwgpo7tR4d2pbFu+EBon46hzjHUOYY6x1DnGOocJ+ut85kYdzKzY4E9wBp339LCY5I6rFy5stBDaBXUOUZzdt6+p5KHX1zLPfNX8tzijbjDxGG9+P4HjuKCowfSo0vprRs+ENqnY6hzDHWOoc4x1DmGOsfJeutG1xib2ZNAW6AzMBAoB65295JakKs1xiLZUVXt/H3RBu6Zv4qHX1zLrooqhpZ14QPHDeIDxw1ieJ9DCj1EERERESkyTVpj7O5n1HqyU4BfA3U+oYhIS3l17VbunreKexes4q2te+jeqR0fmDCIi44bxMRhvbRuWEREREQOSj6nUu/H3Z81s0+3xGCkfs8++ywAp5xySoFHUtrUOcaBdF63bTf3LVjNn+at4pU1W2nXxpg6th/ffu8gzji8H53at23p4Waa9ukY6hxDnWOocwx1jqHOcbLeOq+JsZmdD4wDOuVs1nnHgdauXVvoIbQK6hyjsc679lbx6MtruXveKp55Yz3VDscO7sF33jeOC44ZQO+uHYNGmn3ap2Oocwx1jqHOMdQ5hjrHyXrrfNYY/xroApwB3ABcDMxy92tafnhxtMZYpLCqq50ZSzdyz7xVPPTiWrbvqWRQz868/7iBfOC4wYzu17XQQxQRERGRDGvSGmPgJHc/xswWuvt3zOw/gbubd4gi0lotWrctXTe8mlWbd9G1Yzvec1R/LpowmONHlNGmjdYNi4iIiEjLymdivCv9faeZDQQ2AiNabkhSl7/97W8AnH766QUeSWlT5xiPPvEkL6/eyl+39Gbhyi20MThtTF++eu5Yzj6yP507aN1wc9E+HUOdY6hzDHWOoc4x1DlO1lvnMzG+38x6Aj8F5gFOckr1QUuf7wbgqPT5Pga8BvwRGA4sAy5x903p/b8BXANUAV9w90fS7ROBm0neSupB4Fp3dzPrCNwKTCSZyF/q7suaMuZC27hxY6GH0Cqoc8taUb6TG/++lDfnvERltVPZp4xvnX8E7xs/kH7dOjX+BHLAtE/HUOcY6hxDnWOocwx1jpP11o2uMd7vzsmEs5O7b2nSFzW7BXjG3W8wsw4ka5j/GSh39x+Z2deBXu7+NTM7ErgDmELyPsqPA2PcvcrMZgHXAjNIJsY/d/eHzOyzwDHu/mkzuwz4gLtf2tCYtMZYpOXMX76JG55ZykMvrqGNGe89diDXnDKCowb1KPTQRERERKSVaNIaYzM7y90fB3D3PWbW3czudPfLDnIw3YHTgKvS59wL7DWzC4Gp6d1uAZ4CvgZcCNzp7nuApWa2CJhiZsuA7u4+PX3eW4H3Aw+lj/l2+lx3Ab80M/MD+SmAiDRJVbXz+CtvccMzS5i9bBPdOrXjE6eN5KqThjOgR+dCD09EREREZJ98TqX+tpn1dfc7zOxq4Drgu034miOB9cBNZnYsMJfkqO+h7r4GwN3XmFm/9P6DSI4I11iZbqtIP669veYxK9LnqjSzLUBvYEMTxl1QTz75JABnnHFGgUdS2tS56XbureSuuSu58dmlLNu4k8G9OvOvFxzJJZOH0LVj8leOOsdR6xjqHEOdY6hzDHWOoc5xst46n4nxucA0M7sOeB44xd3Lm/g1JwCfd/eZZvbfwNcbuH9dl6T1BrY39Jj9n9jsk8AnAYYOHdrQmAtu69athR5Cq6DOB2/d1t3cMn0Zt89czuadFYwf0pP/Oedwzhl3KO3attnvvuocR61jqHMMdY6hzjHUOYY6x8l663zex7gM6AD8Fngd+F76uIOaHJtZf2CGuw9PPz+VZGI8GpiaHi0eADzl7mPTC2/h7j9M7/8IyWnSy4An3f3wdPuH0sd/quY+7j7dzNoBa4G+DZ1KrTXGIgfntbXb+O0zS7hvwWoqqqs5+8hD+cSpI5k4rBdmeqslERERESkOTX0f47m8fYT2KOCi9PORBzMYd19rZivMbKy7vwacCbyc/roS+FH6+73pQ+4D/mBmPyO5+NZhwKz04lvbzOwEYCZwBfCLnMdcCUwHLgb+qvXFIs3H3Xl20QZ++8xSnn59PZ3at+HSyUP42CkjGNHnkEIPT0RERETkgDQ6MXb3lnjP4s8Dt6dXpF4CXA20ITll+xpgOfDB9Ou/ZGbTSCbOlcDn3L0qfZ7P8PbbNT2U/gL4HXBbeqGucuCgLhRWTB5//HEAzjrrrAKPpLSpc8P2VlZz3/OrueGZJby6dht9u3XkK2eP4fLjh9HrkA55P486x1HrGOocQ51jqHMMdY6hznGy3jqfq1JfVNd2d7/7YL+ouy8A6jqEfWY99/8+8P06ts8hOYpde/tu0ol1qdi1a1ehh9AqqHPdNu/cy+0zl3PLc8tYt20PYw/txk8uPoYLxw+kY7u2B/x86hxHrWOocwx1jqHOMdQ5hjrHyXrrfNYYVwCvAHN4+6JW7u4fa+GxhdIaY5F3enPjDm58dinT5qxkV0UVpx7Wh4+fOpLTDuuj9cMiIiIikilNXWN8FPDvQFfgX9J1wSJSwua+Wc5vn17KIy+vpV0b48Lxg/j4qSM4vH/3Qg9NRERERKTZ5bPG+DXgEjObAPzMzFaTXPF5VYuPTvZ59NFHATj77LMLPJLS1po7V1ZV88hLb3HDs0uYv3wzPTq35zOnj+LKk4ZzaPdOzfq1WnPnaGodQ51jqHMMdY6hzjHUOU7WW+ezxvgXvP0ewEuA04E3gC4tOC6ppaKiotBDaBVaY+cdeyqZNmcFN/59KSvKdzG0rAvfed84PjhpMF065HNSyYFrjZ0LRa1jqHMMdY6hzjHUOYY6x8l663zWGF9Z13Z3v6VFRlQgWmMsrc3aLbu5+bll/GHmm2zdXcnEYb34xKkjePeR/WnbRuuHRURERKS0NGmNcalNgEVau5dXb+WGZ5Zw3/OrqXbn3KP68/FTRzJhaK9CD01EREREpCDyOZV6KW+fSg3Jland3Ue22KjkHR5++GEAzj333AKPpLSVamd356nX13PDM0v4+6KNdOnQlo+cMIxrThnBkLL4VRGl2rkYqXUMdY6hzjHUOYY6x1DnOFlvnc8Cwkkkk+G/Ame07HBEpDntrqji3gWruOGZpbyxbjuHdu/I1849nA9PGUqPLu0LPTwRERERkaLQ6BrjfXc0m+fuE1p4PAWjNcZSSsp37OX3M97k1unL2LB9L0cM6M4nTh3BBccMpEO7NoUenoiIiIhIuCatMTazsvTDtmbWi+ToMe5e3nxDFJHmsGT9dn737FL+NG8luyuqmTq2L584dSQnjeqNmS6oJSIiIiJSl3xOpZ5LssbYgHnpNge0xjjQAw88AMD5559f4JGUtix2dndmLS3nt88s5YlX36J9mzZ84LhBfPzUERx2aLdCD69OWeycVWodQ51jqHMMdY6hzjHUOU7WW+dzVeoREQORhrVvr/WgEbLUubKqmgdfXMsNzyxh4cot9OrSns+fMZqPnjicvt06Fnp4DcpS56xT6xjqHEOdY6hzDHWOoc5xst46n/cxng3cBPzB3TdHDKoQtMZYsmLb7gr+OHsFN/19Gas272Jkn0P42Ckj+IcJg+ncoW2hhyciIiIiUpSatMYY+BBwNTDHzOaQTJIf9Xyv2iUizWL15l3c9Pel3DlrBdv2VDJlRBnfft84zjy8H23aaP2wiIiIiMjByudU6kXAN83sX4ALgBuBajO7EfhvXYQrxl/+8hcA3vve9xZ4JKWtGDu/sHILv31mCQ+8sAaA844ewCdOHcExg3sWdmBNUIydS5Vax1DnGOocQ51jqHMMdY6T9db5HDHGzI4hOWp8HvAn4HbgFJL3Nh7fUoOTt3Xu3LnQQ2gViqVzdbXz11fX8dtnljBzaTldO7bj6pOGc9XJwxncq0uhh9dkxdK5NVDrGOocQ51jqHMMdY6hznGy3jqfNcZzgc3A74A/ufuenNvudveLWnSEQbTGWIrB7ooq/jRvJb97dilL1u9gYI9OXH3yCC6dMoTunbJ9QQMRERERkUJq6hrjD7r7krpuKJVJsUihbdi+h1unv8nvZ7xJ+Y69HD2oB/992XjOO3oA7du2KfTwRERERERKWj5rjOucFEuse++9F4ALL7ywwCMpbdGdF63bxg3PLOXu+avYW1nNWUf04+OnjuT4EWWYle4FtbQ/x1HrGOocQ51jqHMMdY6hznGy3jqvNcZSeN27dy/0EFqFiM7uzvTFG/ntM0t48rX1dGzXhosnDuaaU0Ywqm/XFv/6xUD7cxy1jqHOMdQ5hjrHUOcY6hwn660bXWPcWmiNsbS0iqpq7l+4mhueWcpLq7fSp2sHPnrCcD5ywlB6d+1Y6OGJiIiIiJS0Jq0xNrP2wGeA09JNfwN+7e4VzTdEkdK1ZVcFd8xazs1/X8barbsZ3a8rP7roaN5/3CA6tW9b6OGJiIiIiLR6+ZxK/SugPfC/6ecfTbd9vKUGJe909913A3DRRbreWUtqzs4ryndy49+XMm32CnbsreKkUb354UVHc/qYvrRpU7rrh/Oh/TmOWsdQ5xjqHEOdY6hzDHWOk/XW+UyMJ7v7sTmf/9XMnm+pAUndevfuXeghtArN0Xn+8k3c8MxSHnpxDW3MeO+xA7nmlBEcNahHM4ywNGh/jqPWMdQ5hjrHUOcY6hxDneNkvXU+72M8j+Qtmxann48E7nL3CQHjC6M1xtIUVdXOYy+/xQ3PLGHOm5vo1qkdHz5+KFedNJwBPbL9ZuciIiIiIqWgqe9jfB3wpJktAQwYBlzdjOMTyaydeyu5a+5Kbnx2Kcs27mRwr8786wVHcsnkIXTtqIu+i4iIiIhkQT7vY/yEmR0GjCWZGL/q7ntafGSyn7vuuguAiy++uMAjKW35dl63dTe3TF/G7TOXs3lnBeOH9OR/zjmcc8YdSru2bSKGmmnan+OodQx1jqHOMdQ5hjrHUOc4WW+dz1Wpv1xr05lmhrv/rIXGJHXo379/oYfQKjTW+bW12/jtM0u4b8FqKqqrOfvIQ/nEqSOZOKwXZq37gloHQvtzHLWOoc4x1DmGOsdQ5xjqHCfrrfNZY7wG+HXt7e7+nZYaVCFojbHUx9155o0N/PaZJTzzxgY6t2/LBycN5mMnj2B4n0MKPTwREREREclDU9cYrym1SbBIPvZWVnPf86u54ZklvLp2G327deS6c8Zy+fFD6dmlQ6GHJyIiIiIizSSfifFIM/szsBtYDfzd3f/U1C9sZm2BOcAqd7/AzMqAPwLDgWXAJe6+Kb3vN4BrgCrgC+7+SLp9InAz0Bl4ELjW3d3MOgK3AhOBjcCl7r6sqWMupGnTpgFwySWXFHgkpW3atGnsraxmy4BJ3PLcMtZt28PYQ7vx04uP4X3jB9KxXdtCD7EkaH+Oo9Yx1DmGOsdQ5xjqHEOd42S9dT4T4wuBtiSTz4HAx83sNHe/tolf+1rgFaB7+vnXgSfc/Udm9vX086+Z2ZHAZcC49Os/bmZj3L0K+BXwSWAGycT4XOAhkkn0JncfbWaXAT8GLm3ieAtq8ODBhR5Cq7CmsgtPvrqeBQtf49TD+vAfHzyWUw/ro/XDzUz7cxy1jqHOMdQ5hjrHUOcY6hwn660bXWP8jgckR3pvdffLD/qLmg0GbgG+D3w5PWL8GjDV3deY2QDgKXcfmx4txt1/mD72EeDbJEeVn3T3w9PtH0of/6ma+7j7dDNrB6wF+noD36zWGLdu1dXO9x98hd89u5QzD+/HdeeO5fD+3Rt/oIiIiIiIZEKT1hib2QiSdca7003tgW82cUz/D/gq0C1n26HuvgYgnRz3S7cPIjkiXGNluq0i/bj29prHrEifq9LMtgC9gQ1NHLeUoN0VVfzT/z3PAwvXcPXJw/mX84+kTRsdIRYRERERaS3yecPV/wOqcz6vTrcdFDO7AFjn7nPzfUgd27yB7Q09pvZYPmlmc8xszvr16/McTmHccccd3HHHHYUeRsnZsrOCK26cxQML1/DN845gzPaF/PGPdxZ6WCVP+3MctY6hzjHUOYY6x1DnGOocJ+ut81lj3M7d99Z84u57zawpl+Q9GXifmZ0HdAK6m9nvgbfMbEDOqdTr0vuvBIbkPH4wyUXAVqYf196e+5iV6anUPYDy2gNx9+uB6yE5lboJ31OLGzFiRKGHUHJWb97FVTfNYumGHfz3ZeO5cPwgZsxY1/gDpcm0P8dR6xjqHEOdY6hzDHWOoc5xst46n/cxfgz4hbvfl35+IcmVoc9s8hc3mwp8JV1j/FNgY87Ft8rc/atmNg74AzCF5OJbTwCHuXuVmc0GPg/MJLn41i/c/UEz+xxwtLt/Or341kXu3uDl0bTGuHV5Zc1WrrppFjv3VPGbKyZy0qg+hR6SiIiIiIi0oKa+j/GngdvN7JckpyivAK5oxvHV+BEwzcyuAZYDHwRw95fMbBrwMlAJfC69IjXAZ3j77ZoeSn8B/A64zcwWkRwpvqwFxisZ9dziDXzq1rkc0rEd//eZE3WRLRERERGRVi7vq1KbWdf0/ttadkiFUexHjG+//XYALr/8oC8GLsC9C1bxlf97nhF9DuHmq6cwsGfn/W5X5xjqHEetY6hzDHWOoc4x1DmGOsfJQuumXpX6y7U+B8Ddf9Yso5O8jBkzptBDyDR357fPLOEHD77K8SPKuP6KSfTo3P4d91PnGOocR61jqHMMdY6hzjHUOYY6x8l663zWGG8mec/ge3K3u/t3WmxUBVDsR4zl4FVVO/9+/8vc/Nwyzj9mAD+75Fg6tmtb6GGJiIiIiEigpq4xHgl8AzgT+K67P96cgxNpSbsrqvjSHxfw0Itr+fgpI/jn847QexSLiIiIiMh+Gn0fY3cvd/frSC5g9UEze9jMJrf80CTXrbfeyq233lroYWTK5p17+ejvZvLwS2v51vlH8K0Ljmx0UqzOMdQ5jlrHUOcY6hxDnWOocwx1jpP11vmsMf4LUHO+tQFDgRmAzkUNNG7cuEIPIVNWbtrJVTfNZvnGnfziQ8dxwTED83qcOsdQ5zhqHUOdY6hzDHWOoc4x1DlO1lvns8b49Lq2u/vfWmREBaI1xqXjpdVbuPqm2eyqqOK3V0zihJG9Cz0kEREREREpsCatMS61CbCUtmff2MCnfz+Xbp3a8afPnMSYQ7sVekgiIiIiIlLk6p0Ym9lS3j6Fer+bAHf3kS02KnmHm2++GYCrrrqqoOMoZvfMX8l1/7eQ0f26ctPVkxnQo3PjD6pFnWOocxy1jqHOMdQ5hjrHUOcY6hwn660bOmKce4jZgL8CZ7TscKQ+48ePL/QQipa786u/LeYnD7/GiSN785srJtK90zvfozgf6hxDneOodQx1jqHOMdQ5hjrHUOc4WW/d6BrjfXc0m+fuE1p4PAWjNcbZVFXtfOcvL3Hr9Dd537ED+ekHj9F7FIuIiIiIyDs09X2MMbORJEeNpUCqqqoAaNtWk74auyuquPbO+Tzy0lt86rSRfO3cw5v8HsXqHEOd46h1DHWOoc4x1DmGOsdQ5zhZb93QGuMXSNYYdwS6AJ+KGpS802233QZk95z95rZpx14+fusc5i3fxL+990iuPnlEszyvOsdQ5zhqHUOdY6hzDHWOoc4x1DlO1ls3dMT4gvT33e7+VsRgpH4TJpTsWewHbEX5Tq68aRYrN+3ifz88gfccPaDZnludY6hzHLWOoc4x1DmGOsdQ5xjqHCfrrfNeY1zqtMY4G15ctYWrbppNRVU1v71iElNGlBV6SCIiIiIikgFNXmMshVdRUQFA+/YHd7XlUvD06+v5zO/n0rNLB+785PGM7tf871GszjHUOY5ax1DnGOocQ51jqHMMdY6T9dZtCj0Ayc/tt9/O7bffXuhhFMxdc1fysZtnM7T3Idz92ZNaZFIM6hxFneOodQx1jqHOMdQ5hjrHUOc4WW+tI8YZMWlSnUf8S567879PLeanj7zGyaN78+uPTKTbQb5HcT5aa+do6hxHrWOocwx1jqHOMdQ5hjrHyXprrTFOaY1x8amsqubf7nuJ22cu5/3jB/KTi4+lQzud5CAiIiIiIgdOa4xLwO7duwHo1KlTgUcSY9feKj5/x3wef+UtPjN1FNedPbbJ71Gcj9bWuVDUOY5ax1DnGOocQ51jqHMMdY6T9dY6/JYRd955J3feeWehhxGifMdePnzDDJ549S2+e+E4vnbu4SGTYmhdnQtJneOodQx1jqHOMdQ5hjrHUOc4WW+tI8YZcfzxxxd6CCGWb0zeo3j15l386vKJnHtU/9Cv31o6F5o6x1HrGOocQ51jqHMMdY6hznGy3lprjFNaY1x4L6zcwtU3z6Ky2vndlZOYOEzvUSwiIiIiIs1Da4xLwM6dOwHo0qVLgUfSMp58bR2fu30evbp04M6PTWF0v64FGUepdy4W6hxHrWOocwx1jqHOMdQ5hjrHyXprrTHOiGnTpjFt2rRCD6NFTJuzgo/fMocRfQ7hns+eVLBJMZR252KiznHUOoY6x1DnGOocQ51jqHOcrLfWEeOMOPHEEws9hGbn7vz8iUX81+Ovc+phffjVRybStWNhd8lS7FyM1DmOWsdQ5xjqHEOdY6hzDHWOk/XWWmOc0hrjWJVV1fzLvS9yx6wVXDRhED/+h2No31YnMIiIiIiISMvQGuMSsH37dgC6di3cacbNZefeSv7xD/P566vr+MczRvNPZ4/BLObtmBpTSp2LmTrHUesY6hxDnWOocwx1jqHOcbLeWofoMuKuu+7irrvuKvQwmmzD9j186PoZPPXaOr73/qP4yjlji2ZSDKXTudipcxy1jqHOMdQ5hjrHUOcY6hwn6611xDgjTjnllEIPocmWbdjBlTfN4q2tu/n1RyZy9rjY9yjORyl0zgJ1jqPWMdQ5hjrHUOcY6hxDneNkvbXWGKe0xrhlLVixmWtunk21OzdcOZmJw3oVekgiIiIiItKKaI1xCdiyZQsAPXr0KPBIDtxfX32Lz90+nz7dOnDL1VMY2bd41x1kuXOWqHMctY6hzjHUOYY6x1DnGOocJ+utw9cYm9kQM3vSzF4xs5fM7Np0e5mZPWZmb6S/98p5zDfMbJGZvWZm5+Rsn2hmL6S3/dzSxapm1tHM/phun2lmw6O/z+Z2zz33cM899xR6GAfszlnL+cStcxndryt3f+bkop4UQ3Y7Z406x1HrGOocQ51jqHMMdY6hznGy3roQR4wrgX9y93lm1g2Ya2aPAVcBT7j7j8zs68DXga+Z2ZHAZcA4YCDwuJmNcfcq4FfAJ4EZwIPAucBDwDXAJncfbWaXAT8GLg39LpvZaaedVughHBB3578ef4OfP/EGp4/py/9ePoFDCvwexfnIWuesUuc4ah1DnWOocwx1jqHOMdQ5TtZbF3yNsZndC/wy/TXV3deY2QDgKXcfa2bfAHD3H6b3fwT4NrAMeNLdD0+3fyh9/Kdq7uPu082sHbAW6OsNfLNaY9x8Kqqq+eY9LzBtzko+OHEwP7joaL1HsYiIiIiIFFTRrjFOT3E+DpgJHOruawDSyXG/9G6DSI4I11iZbqtIP669veYxK9LnqjSzLUBvYEOtr/9JkiPODB06tNm+r5awadMmAHr1Ku6LVu3YU8nn/jCPp15bzxfeNZovvbt43qM4H1npnHXqHEetY6hzDHWOoc4x1DmGOsfJeuuCHcYzs67An4AvuvvWhu5axzZvYHtDj9l/g/v17j7J3Sf17du3sSEX1L333su9995b6GE0aP22PVx2/Qyefn09P/jA0Xz57OJ6j+J8ZKFzKVDnOGodQ51jqHMMdY6hzjHUOU7WWxfkiLGZtSeZFN/u7nenm98yswE5p1KvS7evBIbkPHwwsDrdPriO7bmPWZmeSt0DKG+RbybI1KlTCz2EBi1Zv52rbprNum27+e0VkzjziEMLPaSDUuydS4U6x1HrGOocQ51jqHMMdY6hznGy3jp8jXF65ehbgHJ3/2LO9p8CG3MuvlXm7l81s3HAH4ApJBffegI4zN2rzGw28HmSU7EfBH7h7g+a2eeAo9390+nFty5y90saGpfWGB+8ecs3cc3NszEzbrxqMuOH9Cz0kERERERERPZTbGuMTwY+CrxgZgvSbf8M/AiYZmbXAMuBDwK4+0tmNg14meSK1p9Lr0gN8BngZqAzydWoH0q3/w64zcwWkRwpvqyFv6cWt2FDsjy6T58+BR7J/h57+S0+f8c8Du3eiVuunsLwPocUekhNUqydS406x1HrGOocQ51jqHMMdY6hznGy3jp8Yuzuz1L3GmCAM+t5zPeB79exfQ5wVB3bd5NOrEvF/fffD8BVV11V2IHkuH3mm/zLn1/kqEE9uPGqyfTp2rHQQ2qyYuxcitQ5jlrHUOcY6hxDnWOocwx1jpP11sX/xrICwJln1vkzg4Jwd/7z0df55ZOLOGNsX/7n8gl06VAau1IxdS5l6hxHrWOocwx1jqHOMdQ5hjrHyXrrgr+PcbHQGuP8VFRV8/U/vcCf5q3ksslD+N77j6Kd3qNYRERERESKXLGtMZaDsG5dcpHufv36NXLPlrN9TyWfvX0eT7++ni+edRjXnnlY5t6OqTHF0Lk1UOc4ah1DnWOocwx1jqHOMdQ5TtZb61BfRjz44IM8+OCDBfv667bt5rLrp/P3RRv48T8czRfPGlNyk2IofOfWQp3jqHUMdY6hzjHUOYY6x1DnOFlvrVOpU8V+KvWqVasAGDRoUPjXXrx+O1feOIuN2/fyv5dP4IzDs/lToHwUsnNros5x1DqGOsdQ5xjqHEOdY6hznCy0buhUak2MU8U+MS6UuW+Wc80tc2ibvkfxsXqPYhERERERySCtMS4Ba9euBaB///5hX/ORl9byhTvmM6BHJ2752BSG9c72exTnoxCdWyN1jqPWMdQ5hjrHUOcY6hxDneNkvbXWGGfEww8/zMMPPxz29W6bvozP/H4uRwzozp8+c1KrmBRDfOfWSp3jqHUMdY6hzjHUOYY6x1DnOFlvrVOpU8V+KnXUT2DcnZ888hq/emoxZx3Rj198aAKdO7Rt0a9ZTLL+k66sUOc4ah1DnWOocwx1jqHOMdQ5ThZaa41xHop9Yhxhb2U1X//TQu6ev4oPHz+U775vnN6jWERERERESoLWGJeAlr7K27bdFXzm9/N4dtEGvnL2GD53xuiSfDumxmThanqlQJ3jqHUMdY6hzjHUOYY6x1DnOFlvrcOBGfHYY4/x2GOPtchzv7V1N5f8ZgbTl2zkpxcfwz++67BWOSmGlu0sb1PnOGodQ51jqHMMdY6hzjHUOU7WW+tU6lSxn0q9bt06APr1a973EF60bhtX3jibTTv38quPTOT0MX2b9fmzpqU6y/7UOY5ax1DnGOocQ51jqHMMdY6ThdZaY5yHYp8Yt4TZy8r5+C1zaN+2DTddNZmjB/co9JBERERERERahNYYl4AVK1YAMGTIkGZ5vodeWMO1f1zA4J6dueVjUxhS1qVZnjfrmruz1E2d46h1DHWOoc4x1DmGOsdQ5zhZb601xhnxxBNP8MQTTzTLc93896V89g/zOGpgd+76zEmaFOdozs5SP3WOo9Yx1DmGOsdQ5xjqHEOd42S9tU6lThX7qdQbNmwAoE+fPgf9HNXVzo8ffpXfPL2Edx95KD+/7LhW9R7F+WiOztI4dY6j1jHUOYY6x1DnGOocQ53jZKG11hjnodgnxk21p7KK6/5vIfc9v5qPnDCU77zvKNq2aZ1XnhYRERERkdZHa4xLwLJlywAYPnz4AT926+4KPn3bXJ5bvJHrzhnLZ6eOarVvx9SYpnSW/KlzHLWOoc4x1DmGOsdQ5xjqHCfrrbXGOCOeeuopnnrqqQN+3Notu7nk19OZtbScn11yLJ87Y7QmxQ042M5yYNQ5jlrHUOcY6hxDnWOocwx1jpP11jqVOlXsp1Jv2rQJgF69euX9mNff2sZVN85iy64Kfv3RiZx6WOt+j+J8HExnOXDqHEetY6hzDHWOoc4x1DmGOsfJQmutMc5DsU+MD9SMJRv55K1z6Ni+LTddNZmjBuk9ikVEREREpPXSGuMSsGTJEgBGjhzZ6H3vX7iaL//xeYaUdebmq/UexQfiQDrLwVPnOGodQ51jqHMMdY6hzjHUOU7WW2tinBFPP/000PiO9rtnl/K9B15m4tBe3HDlJHp26RAxvJKRb2dpGnWOo9Yx1DmGOsdQ5xjqHEOd42S9tU6lThX7qdRbtmwBoEePuk+Jrq52fvDgK9zw7FLOGXco/33ZcXRqr/coPlCNdZbmoc5x1DqGOsdQ5xjqHEOdY6hznCy01hrjPBT7xLgheyqr+Kdpz3P/wjVceeIw/vW94/QexSIiIiIiIjm0xrgELFq0CIDRo0fvt33Lrgo+eescZi4t5+vvOZxPnTZSb8fUBPV1lualznHUOoY6x1DnGOocQ51jqHOcrLfWxDgjnn32WWD/HW315l1cddMslm7Ywf+7dDzvP25QoYZXMurqLM1PneOodQx1jqHOMdQ5hjrHUOc4WW+tU6lTxX4q9fbt2wHo2rUrAK+u3cpVN85m+55KfvPRiZw8uk8hh1cyaneWlqHOcdQ6hjrHUOcY6hxDnWOoc5wstNap1CUgdwd7bvEGPnXrXLp0bMu0T53IkQO7F3BkpaWY/yCXEnWOo9Yx1DmGOsdQ5xjqHEOd42S9dUlPjM3sXOC/gbbADe7+owIP6aC99tprye+7u/GVac8ztHcXbvnYFAb17FzgkZWWms5jx44t8EhKmzrHUesY6hxDnWOocwx1jqHOcbLeumQnxmbWFvgf4N3ASmC2md3n7i8XdmQHZ/r06azZspvfrBnClOFl/PaKSfTo0r7Qwyo506dPB7L7Bzor1DmOWsdQ5xjqHEOdY6hzDHWOk/XWJbvG2MxOBL7t7uekn38DwN1/WNf9i3mNsbvz3T/P5w8zl3Pm0UP42SXj9R7FLWTnzp0AdOnSpcAjKW3qHEetY6hzDHWOoc4x1DmGOsfJQuvWusZ4ELAi5/OVwPG5dzCzTwKfBBg6dGjcyA6QmdHtkEP48MmH8S/nH0kbvUdxiynmP8ilRJ3jqHUMdY6hzjHUOYY6x1DnOFlvXcoT47pmj/sdHnf364HrITliHDGog3Xu4CrANCluYa+88goARxxxRIFHUtrUOY5ax1DnGOocQ51jqHMMdY6T9dalPDFeCQzJ+XwwsLpAY2myWbNmAXDkkUcWeCSlbebMmUB2/0BnhTrHUesY6hxDnWOocwx1jqHOcbLeupTXGLcDXgfOBFYBs4EPu/tLdd2/mNcYA+zevRuATp06FXgkpU2dY6hzHLWOoc4x1DmGOsdQ5xjqHCcLrVvlGmN3rzSzfwQeIXm7phvrmxRnQTHvYKVEnWOocxy1jqHOMdQ5hjrHUOcY6hwn661LdmIM4O4PAg8WehzN4cUXXwTgqKOOKvBISps6x1DnOGodQ51jqHMMdY6hzjHUOU7WW5f0xLiU1JzmndUdLSvUOYY6x1HrGOocQ51jqHMMdY6hznGy3rpk1xgfqGJfY1xRUQFA+/btCzyS0qbOMdQ5jlrHUOcY6hxDnWOocwx1jpOF1q1yjXGpKeYdrJSocwx1jqPWMdQ5hjrHUOcY6hxDneNkvXWbQg9A8rNw4UIWLlxY6GGUPHWOoc5x1DqGOsdQ5xjqHEOdY6hznKy31hHjjJg3bx4AxxxzTIFHUtrUOYY6x1HrGOocQ51jqHMMdY6hznGy3lprjFPFvsa4qqoKgLZt2xZ4JKVNnWOocxy1jqHOMdQ5hjrHUOcY6hwnC621xrgEFPMOVkrUOYY6x1HrGOocQ51jqHMMdY6hznGy3lprjDNiwYIFLFiwoNDDKHnqHEOd46h1DHWOoc4x1DmGOsdQ5zhZb62JcUZkfUfLCnWOoc5x1DqGOsdQ5xjqHEOdY6hznKy31hrjlJmtB94s9Dga0QfYUOhByDvodSlOel2Kj16T4qTXpTjpdSk+ek2Kk16X4lSsr8swd+9b1w2aGGeImc2pb7G4FI5el+Kk16X46DUpTnpdipNel+Kj16Q46XUpTll8XXQqtYiIiIiIiLRqmhiLiIiIiIhIq6aJcbZcX+gBSJ30uhQnvS7FR69JcdLrUpz0uhQfvSbFSa9Lccrc66I1xiIiIiIiItKq6YixiIiIiIiItGqaGGeAmZ1rZq+Z2SIz+3qhx1OKzGyImT1pZq+Y2Utmdm26vczMHjOzN9Lfe+U85hvpa/KamZ2Ts32imb2Q3vZzM7N0e0cz+2O6faaZDQ//RjPIzNqa2Xwzuz/9XK9JgZlZTzO7y8xeTf/MnKjXpfDM7Evp318vmtkdZtZJr0s8M7vRzNaZ2Ys520JeBzO7Mv0ab5jZlUHfctGr5zX5afp32EIzu8fMeubcptckQF2vS85tXzEzN7M+Odv0ugSo73Uxs8+n7V8ys5/kbC+d18Xd9auIfwFtgcXASKAD8DxwZKHHVWq/gAHAhPTjbsDrwJHAT4Cvp9u/Dvw4/fjI9LXoCIxIX6O26W2zgBMBAx4C3pNu/yzw6/Tjy4A/Fvr7zsIv4MvAH4D708/1mhT+NbkF+Hj6cQegp16Xgr8mg4ClQOf082nAVXpdCvJanAZMAF7M2dbirwNQBixJf++Vftyr0D2K4Vc9r8nZQLv04x/rNSmO1yXdPgR4BHgT6KPXpfCvC3AG8DjQMf28Xym+LjpiXPymAIvcfYm77wXuBC4s8JhKjruvcfd56cfbgFdI/qN5IckkgPT396cfXwjc6e573H0psAiYYmYDgO7uPt2TP+W31npMzXPdBZxZ89MzqZuZDQbOB27I2azXpIDMrDvJP5q/A3D3ve6+Gb0uxaAd0NnM2gFdgNXodQnn7k8D5bU2R7wO5wCPuXu5u28CHgPObe7vL4vqek3c/VF3r0w/nQEMTj/WaxKknj8rAP8FfBXIvRCSXpcg9bwunwF+5O570vusS7eX1OuiiXHxGwSsyPl8ZbpNWkh6SsdxwEzgUHdfA8nkGeiX3q2+12VQ+nHt7fs9Jv3HeAvQu0W+idLx/0j+cazO2abXpLBGAuuBmyw5xf0GMzsEvS4F5e6rgP8AlgNrgC3u/ih6XYpFxOug/y8cvI+RHNECvSYFZWbvA1a5+/O1btLrUlhjgFPTU5//ZmaT0+0l9bpoYlz86vppvC4l3kLMrCvwJ+CL7r61obvWsc0b2N7QY6QOZnYBsM7d5+b7kDq26TVpfu1ITrH6lbsfB+wgOTW0PnpdAliyZvVCklPZBgKHmNlHGnpIHdv0usRrztdBr89BMLNvApXA7TWb6ribXpMAZtYF+Cbwr3XdXMc2vS5x2pGc3nwCcB0wLT3KW1KviybGxW8lyVqLGoNJTo+TZmZm7Ukmxbe7+93p5rfS00FIf685daS+12Ulb5+Olbt9v8ekpzr2oO5TiCRxMvA+M1tGsoTgXWb2e/SaFNpKYKW7z0w/v4tkoqzXpbDOApa6+3p3rwDuBk5Cr0uxiHgd9P+FA5Re3OcC4PL0dE/Qa1JIo0h+uPd8+m//YGCemfVHr0uhrQTu9sQskjP5+lBir4smxsVvNnCYmY0wsw4ki9TvK/CYSk76U6/fAa+4+89ybroPuDL9+Erg3pztl6VX1hsBHAbMSk+R22ZmJ6TPeUWtx9Q818XAX3P+IZZa3P0b7j7Y3YeT7Pd/dfePoNekoNx9LbDCzMamm84EXkavS6EtB04wsy5pzzNJrpWg16U4RLwOjwBnm1mv9AyCs9NtUgczOxf4GvA+d9+Zc5NekwJx9xfcvZ+7D0//7V9JcmHUteh1KbQ/A+8CMLMxJBfe3ECpvS5eBFc/06+GfwHnkVwleTHwzUKPpxR/AaeQnK6xEFiQ/jqPZM3DE8Ab6e9lOY/5ZvqavEZ6pb10+yTgxfS2XwKWbu8E/B/JhQlmASML/X1n5RcwlbevSq3XpPCvx3hgTvrn5c8kp1fpdSn86/Id4NW06W0kVwnV6xL/OtxBss67guQ/9tdEvQ4ka2UXpb+uLnSLYvlVz2uyiGQ944L016/1mhT+dal1+zLSq1LrdSns60IyEf592nke8K5SfF1qBigiIiIiIiLSKulUahEREREREWnVNDEWERERERGRVk0TYxEREREREWnVNDEWERERERGRVk0TYxEREREREWnV2hV6ACIiItHMrAp4IWdTGXCfu/9jgYbUYszsxyTvP/kW8A/uvqfAQxIRESk6ersmERFpdcxsu7t3zfn8KmBSKU6MRUREpHE6lVpERCSHmQ0zsyfMbGH6+9B0+81m9msze8bMXjezC9LtHczsHjN70cxeMLNlOc91sZmVm9kCM1trZl9Jt3875+Ovm9lN6cdlZvbn9GvPMLNjcu6/Kt3+qpm9K2dMF9ca/1fM7Nvpx0+Z2aT04++Z2fY6vt/hZvZirTHfnEeLlWbWNv38M2bmZjY8/fwjZjYr/b5/k3O/7Wb2n2Y2L32+vrXGMip9zAIzq8r5eGD6vbxmZi+nbQamj1lmZn3Sj39f872Y2ZPpY7enj1tgZu8zsylm9pyZzU9/H3tge4iIiJQiTYxFRET290vgVnc/Brgd+HnObcOB04HzgV+bWSfgHKC9ux8FnFHrudoCf3b38cCva38hM7sCOBX4RLrpO8D89Gv/M3Brzt3/K93+O+CCA/mGzKwfcOaBPCbVUItVJN87wIXAovRrHQFcCpycft9VwOXp/Q4B5rn7BOBvwL/lfjF3X+zu49PH7ar52N1Xp3e5HBgHrAcm1foejwaOynmuM9LnmQNcnj7PfcCrwGnufhzwr8APDqKLiIiUGK0xFhER2d+JwEXpx7cBP8m5bZq7VwNvmNkS4HCSiV+XmqOitXQFyuv5OmeRrP093t0r022nAP8A4O5/NbPeZtYjve1LZvYxoB/7T8B/ambfAjYCn6nna/0LyQTwjnpuH2VmC9KPe5BMWqHhFrcBHzWz5cAbwOB0+5nARGC2mQF0Btalt1UDf0w//j1wdz3jqc/tQEdgK/B4rdu+RzLR/n4jz9EDuMXMDgMcaH+AYxARkRKkI8YiIiIN83o+rvn8UWAJyVHMJ2vdPgJYWc/zjgQ+AvzM0hkkYHXcr+Zr/pe7HwlcBvxnzu3XpUdG7wC+XcfjhwNHuftf6hkHQO6R2usauF/u97+WZFJ5HXBTznYDbsk52jvW3esaV+3ny8fl7j4cuA/4Ys72k4DtwPN5PMe/A0+mR/jfC3Q6wDGIiEgJ0sRYRERkf8+RTD4hOXX32ZzbPmhmbcxsFMnE9rX0aO8ukgniviO5ZtaBZOL1QD1f53p3nwYs5e1TqZ9OvyZmNhXY4O5baz1uK9CnjufbCHSoY/u/UeuU5QPQUAtIJsT93H1ezrYngIvT07dr1k0PS29rA9Ssif5wHc+Xr9oNvk1yWnQ+epCcBg5w1UF+fRERKTE6lVpERGR/XwBuNLPrSI4CX51z22skpxkfCnza3Xeb2SVAd3f/Xc1FoFK/J1kP+3/pAeH+QJWZ/aHW1/snYLqZ/YVkgneTmS0EdgJX5tzvS2b2EZJ/u7+Ss/3fzeyLJKcYf4rk9OxcK9396QMJkKOhFrj7A9Sa+Lv7y+mp3Y+aWRugAvgc8CawAxhnZnOBLSRrkQ/E7Wa2i+QHER/O2T7T3RfXXPyrET8hOZX6y8BfD/Dri4hIidLbNYmIiOQhvVLz/e5+V573f8rdp9ba9h/AL919WbMPMAOs1ttkiYiIFAudSi0iItIyvlvHtt+THHkVERGRIqIjxiIiIiIiItKq6YixiIiIiIiItGqaGIuIiIiIiEirpomxiIiIiIiItGqaGIuIiIiIiEirpomxiIiIiIiItGqaGIuIiIiIiEir9v8Byr0otYcBdGkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "unique_words = []\n",
    "step = 10\n",
    "lim = np.linspace(0, len(data), 10)\n",
    "for i in lim:\n",
    "    unique_words.append(len(set(' '.join(data.dropna().loc[0:i, 'lemmatized'].values).split())))\n",
    "    \n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(lim, unique_words)\n",
    "plt.title('Динамика изменения частоты встречаемости новых слов по мере роста корпуса ')\n",
    "plt.xlabel('Порядковый номер твита')\n",
    "plt.ylabel('Число уникальных слов в твитах')\n",
    "\n",
    "for line in list(lim):\n",
    "    plt.axvline(x=line, linestyle=':', color='grey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действительно, в корпусе из нескольких сотен твитов почти в каждом обнаруживаются уникальные для корпуса слова, и по мере увеличения корпуса они встречаются все реже. Однако нельзя сказать, что эта динамика в какой-то момент прекращается, поэтому стоит рассматривать весь корпус целиком."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выделение обучающей и тестовой выборок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем датасет на обучающую и тестовую выборку, приняв объем тестовой за 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data['lemmatized'].to_frame()\n",
    "target = data['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_target, test_target = train_test_split(features, target, test_size=0.25, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удаление слов нейтральной тональности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим к списку стоп-слов названия географических объектов, встречающихся в датасете, названия дней и месяцев. Начнем с извлечения из датасета всех географических объектов. Для демонстрации работы возьмем короткий участок датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "places = geograpy.get_geoPlace_context(text=' '.join(data['text'].sample(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем по несколько стран, регионов и городов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India', 'South Africa', 'United States of America']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Incan', 'Karate', 'News']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Los Angeles', 'Florence', 'Arras']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(places.countries[:3])\n",
    "display(places.regions[:3])\n",
    "display(places.cities[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Добавим к списку стоп-слов только названия стран, так как мноиге выявленные библиотекой `geograpy` города и регионы совпадают с простыми словами, и это ухудшает обобщающую способность модели~~ Добавим только дни недели и месяцы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество стоп-слов до: 179\n",
      "Количество стоп-слов после: 198\n"
     ]
    }
   ],
   "source": [
    "print('Количество стоп-слов до: {}'.format(len(stop_words)))\n",
    "\n",
    "stop_words.update([item.lower() for item in pd.date_range('2022-01-01','2023-01-01', freq='MS').month_name()],\n",
    "                  [item.lower() for item in pd.date_range('2022-01-01','2023-01-01', freq='MS').day_name()]\n",
    "                  \n",
    "                 )\n",
    "# [item.lower() for item in places.countries]\n",
    "print('Количество стоп-слов после: {}'.format(len(stop_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Число стоп-слов значительно увеличилось. Надеемся, на точности предсказания это тоже скажется. Загрузим заранее сформированный список стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"try:\\n    with open('stop_words_v3.txt', 'r') as file:\\n        stop_words = set(file.read().splitlines())\\nexcept:\\n    print('Файл не найден, сформируйте список стоп-слов заново')\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''try:\n",
    "    with open('stop_words_v3.txt', 'r') as file:\n",
    "        stop_words = set(file.read().splitlines())\n",
    "except:\n",
    "    print('Файл не найден, сформируйте список стоп-слов заново')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итого слоп-слов: 198\n"
     ]
    }
   ],
   "source": [
    "print('Итого слоп-слов: {}'.format(len(stop_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем считать, что большая часть - правда нейтральные слова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторизация текста и обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск наилучших гиперпараметров при помощи GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим пайплайн, включающий в себя следующие шаги:\n",
    "* Трансформер, представляющий текст в виде вектора (в качестве трансформера используется мешок слов или TF-IDF)\n",
    "* Модель обучения (рассмотрим логистическую регрессию и метод опорных векторов, другие модели рассматрвиались, но слишком долго обучались и не дали хорошего результата)\n",
    "\n",
    "Также в пайплайн можно было встроить Scaler, однако для текстов все признаки (частоты слов) имеют одинаковый масштаб и это не дало прироста точности\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_ct = ColumnTransformer(transformers=[('count_vectorizer', CountVectorizer(stop_words=stop_words), 'lemmatized')])\n",
    "tf_idf_ct = ColumnTransformer(transformers=[('tf_idf_vectorizer', TfidfVectorizer(stop_words=stop_words), 'lemmatized')])\n",
    "\n",
    "lr_clf = LogisticRegression(class_weight='balanced', random_state=123, solver='sag', max_iter=10000, n_jobs=-1)\n",
    "svm_clf = SVC(class_weight='balanced', random_state=123, max_iter=10000)\n",
    "\n",
    "multi_pipe = Pipeline(steps=[('vectorizer', bow_ct), ('classifier', lr_clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переберем несколько гиперпараметров в поисках наилучшей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 327 candidates, totalling 981 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "324 fits failed out of a total of 981.\n",
      "The score on these train-test partitions for these parameters will be set to 0.0.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "324 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 675, in fit_transform\n",
      "    result = self._fit_transform(X, y, _fit_transform_one)\n",
      "  File \"C:\\anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 606, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"C:\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1048, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 866, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 784, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2077, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1344, in fit_transform\n",
      "    raise ValueError(\"max_df corresponds to < documents than min_df\")\n",
      "ValueError: max_df corresponds to < documents than min_df\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12h 10min 59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params = [{'vectorizer': [tf_idf_ct], \n",
    "           'vectorizer__tf_idf_vectorizer__ngram_range':[(1, 2)], \n",
    "           'classifier': [lr_clf], \n",
    "           'classifier__C': [0.5, 2, 5]\n",
    "          },\n",
    "          \n",
    "          {'vectorizer': [tf_idf_ct], \n",
    "           'vectorizer__tf_idf_vectorizer__ngram_range':[(1,2), (1, 3), (2, 2)],\n",
    "           'vectorizer__tf_idf_vectorizer__max_df':[0.8, 0.9, 1],\n",
    "           'vectorizer__tf_idf_vectorizer__min_df':[2, 5, 10],\n",
    "           'classifier': [svm_clf],\n",
    "           'classifier__C': [0.5, 3, 6, 10],\n",
    "           'classifier__gamma': [0.3, 0.9, 2]\n",
    "          }\n",
    "         ]\n",
    "\n",
    "multi_gs = GridSearchCV(estimator=multi_pipe, param_grid=params, scoring='f1', cv=3, n_jobs=-1, verbose=10, error_score=0.0)\n",
    "multi_gs_res = multi_gs.fit(train_features, train_target).cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем сводную таблицу моделей с разными гиперпараметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>param_vectorizer</th>\n",
       "      <th>param_vectorizer__tf_idf_vectorizer__ngram_range</th>\n",
       "      <th>param_classifier__gamma</th>\n",
       "      <th>param_vectorizer__tf_idf_vectorizer__max_df</th>\n",
       "      <th>param_vectorizer__tf_idf_vectorizer__min_df</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>839.312081</td>\n",
       "      <td>3.841229</td>\n",
       "      <td>297.256259</td>\n",
       "      <td>3.598656</td>\n",
       "      <td>SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123)</td>\n",
       "      <td>3</td>\n",
       "      <td>ColumnTransformer(transformers=[('tf_idf_vectorizer',\\n                                 TfidfVectorizer(max_df=0.8, min_df=2,\\n                                                 ngram_range=(1, 2),\\n                                                 stop_words={'a', 'about',\\n                                                             'above', 'after',\\n                                                             'again', 'against',\\n                                                             'ain', 'all', 'am',\\n                                                             'an', 'and', 'any',\\n                                                             'april', 'are',\\n                                                             'aren', \"aren't\",\\n                                                             'as', 'at',\\n                                                             'august', 'be',\\n                                                             'because', 'been',\\n                                                             'before', 'being',\\n                                                             'below', 'between',\\n                                                             'both', 'but',\\n                                                             'by', 'can', ...}),\\n                                 'lemmatized')])</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>{'classifier': SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123), 'classifier__C': 3, 'classifier__gamma': 0.3, 'vectorizer': ColumnTransformer(transformers=[('tf_idf_vectorizer',\n",
       "                                 TfidfVectorizer(max_df=0.8, min_df=2,\n",
       "                                                 ngram_range=(1, 2),\n",
       "                                                 stop_words={'a', 'about',\n",
       "                                                             'above', 'after',\n",
       "                                                             'again', 'against',\n",
       "                                                             'ain', 'all', 'am',\n",
       "                                                             'an', 'and', 'any',\n",
       "                                                             'april', 'are',\n",
       "                                                             'aren', \"aren't\",\n",
       "                                                             'as', 'at',\n",
       "                                                             'august', 'be',\n",
       "                                                             'because', 'been',\n",
       "                                                             'before', 'being',\n",
       "                                                             'below', 'between',\n",
       "                                                             'both', 'but',\n",
       "                                                             'by', 'can', ...}),\n",
       "                                 'lemmatized')]), 'vectorizer__tf_idf_vectorizer__max_df': 0.9, 'vectorizer__tf_idf_vectorizer__min_df': 2, 'vectorizer__tf_idf_vectorizer__ngram_range': (1, 2)}</td>\n",
       "      <td>0.787381</td>\n",
       "      <td>0.779184</td>\n",
       "      <td>0.774892</td>\n",
       "      <td>0.780486</td>\n",
       "      <td>0.005181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>840.153359</td>\n",
       "      <td>3.484650</td>\n",
       "      <td>297.419488</td>\n",
       "      <td>2.379613</td>\n",
       "      <td>SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123)</td>\n",
       "      <td>3</td>\n",
       "      <td>ColumnTransformer(transformers=[('tf_idf_vectorizer',\\n                                 TfidfVectorizer(max_df=0.8, min_df=2,\\n                                                 ngram_range=(1, 2),\\n                                                 stop_words={'a', 'about',\\n                                                             'above', 'after',\\n                                                             'again', 'against',\\n                                                             'ain', 'all', 'am',\\n                                                             'an', 'and', 'any',\\n                                                             'april', 'are',\\n                                                             'aren', \"aren't\",\\n                                                             'as', 'at',\\n                                                             'august', 'be',\\n                                                             'because', 'been',\\n                                                             'before', 'being',\\n                                                             'below', 'between',\\n                                                             'both', 'but',\\n                                                             'by', 'can', ...}),\\n                                 'lemmatized')])</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>{'classifier': SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123), 'classifier__C': 3, 'classifier__gamma': 0.3, 'vectorizer': ColumnTransformer(transformers=[('tf_idf_vectorizer',\n",
       "                                 TfidfVectorizer(max_df=0.8, min_df=2,\n",
       "                                                 ngram_range=(1, 2),\n",
       "                                                 stop_words={'a', 'about',\n",
       "                                                             'above', 'after',\n",
       "                                                             'again', 'against',\n",
       "                                                             'ain', 'all', 'am',\n",
       "                                                             'an', 'and', 'any',\n",
       "                                                             'april', 'are',\n",
       "                                                             'aren', \"aren't\",\n",
       "                                                             'as', 'at',\n",
       "                                                             'august', 'be',\n",
       "                                                             'because', 'been',\n",
       "                                                             'before', 'being',\n",
       "                                                             'below', 'between',\n",
       "                                                             'both', 'but',\n",
       "                                                             'by', 'can', ...}),\n",
       "                                 'lemmatized')]), 'vectorizer__tf_idf_vectorizer__max_df': 0.8, 'vectorizer__tf_idf_vectorizer__min_df': 2, 'vectorizer__tf_idf_vectorizer__ngram_range': (1, 2)}</td>\n",
       "      <td>0.787381</td>\n",
       "      <td>0.779184</td>\n",
       "      <td>0.774892</td>\n",
       "      <td>0.780486</td>\n",
       "      <td>0.005181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>907.706812</td>\n",
       "      <td>2.957971</td>\n",
       "      <td>315.612825</td>\n",
       "      <td>2.884368</td>\n",
       "      <td>SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123)</td>\n",
       "      <td>3</td>\n",
       "      <td>ColumnTransformer(transformers=[('tf_idf_vectorizer',\\n                                 TfidfVectorizer(max_df=0.8, min_df=2,\\n                                                 ngram_range=(1, 2),\\n                                                 stop_words={'a', 'about',\\n                                                             'above', 'after',\\n                                                             'again', 'against',\\n                                                             'ain', 'all', 'am',\\n                                                             'an', 'and', 'any',\\n                                                             'april', 'are',\\n                                                             'aren', \"aren't\",\\n                                                             'as', 'at',\\n                                                             'august', 'be',\\n                                                             'because', 'been',\\n                                                             'before', 'being',\\n                                                             'below', 'between',\\n                                                             'both', 'but',\\n                                                             'by', 'can', ...}),\\n                                 'lemmatized')])</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>{'classifier': SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123), 'classifier__C': 3, 'classifier__gamma': 0.3, 'vectorizer': ColumnTransformer(transformers=[('tf_idf_vectorizer',\n",
       "                                 TfidfVectorizer(max_df=0.8, min_df=2,\n",
       "                                                 ngram_range=(1, 2),\n",
       "                                                 stop_words={'a', 'about',\n",
       "                                                             'above', 'after',\n",
       "                                                             'again', 'against',\n",
       "                                                             'ain', 'all', 'am',\n",
       "                                                             'an', 'and', 'any',\n",
       "                                                             'april', 'are',\n",
       "                                                             'aren', \"aren't\",\n",
       "                                                             'as', 'at',\n",
       "                                                             'august', 'be',\n",
       "                                                             'because', 'been',\n",
       "                                                             'before', 'being',\n",
       "                                                             'below', 'between',\n",
       "                                                             'both', 'but',\n",
       "                                                             'by', 'can', ...}),\n",
       "                                 'lemmatized')]), 'vectorizer__tf_idf_vectorizer__max_df': 0.8, 'vectorizer__tf_idf_vectorizer__min_df': 2, 'vectorizer__tf_idf_vectorizer__ngram_range': (1, 3)}</td>\n",
       "      <td>0.786163</td>\n",
       "      <td>0.777389</td>\n",
       "      <td>0.774879</td>\n",
       "      <td>0.779477</td>\n",
       "      <td>0.004837</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>903.359438</td>\n",
       "      <td>6.877142</td>\n",
       "      <td>316.130112</td>\n",
       "      <td>2.778566</td>\n",
       "      <td>SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123)</td>\n",
       "      <td>3</td>\n",
       "      <td>ColumnTransformer(transformers=[('tf_idf_vectorizer',\\n                                 TfidfVectorizer(max_df=0.8, min_df=2,\\n                                                 ngram_range=(1, 2),\\n                                                 stop_words={'a', 'about',\\n                                                             'above', 'after',\\n                                                             'again', 'against',\\n                                                             'ain', 'all', 'am',\\n                                                             'an', 'and', 'any',\\n                                                             'april', 'are',\\n                                                             'aren', \"aren't\",\\n                                                             'as', 'at',\\n                                                             'august', 'be',\\n                                                             'because', 'been',\\n                                                             'before', 'being',\\n                                                             'below', 'between',\\n                                                             'both', 'but',\\n                                                             'by', 'can', ...}),\\n                                 'lemmatized')])</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>{'classifier': SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123), 'classifier__C': 3, 'classifier__gamma': 0.3, 'vectorizer': ColumnTransformer(transformers=[('tf_idf_vectorizer',\n",
       "                                 TfidfVectorizer(max_df=0.8, min_df=2,\n",
       "                                                 ngram_range=(1, 2),\n",
       "                                                 stop_words={'a', 'about',\n",
       "                                                             'above', 'after',\n",
       "                                                             'again', 'against',\n",
       "                                                             'ain', 'all', 'am',\n",
       "                                                             'an', 'and', 'any',\n",
       "                                                             'april', 'are',\n",
       "                                                             'aren', \"aren't\",\n",
       "                                                             'as', 'at',\n",
       "                                                             'august', 'be',\n",
       "                                                             'because', 'been',\n",
       "                                                             'before', 'being',\n",
       "                                                             'below', 'between',\n",
       "                                                             'both', 'but',\n",
       "                                                             'by', 'can', ...}),\n",
       "                                 'lemmatized')]), 'vectorizer__tf_idf_vectorizer__max_df': 0.9, 'vectorizer__tf_idf_vectorizer__min_df': 2, 'vectorizer__tf_idf_vectorizer__ngram_range': (1, 3)}</td>\n",
       "      <td>0.786163</td>\n",
       "      <td>0.777389</td>\n",
       "      <td>0.774879</td>\n",
       "      <td>0.779477</td>\n",
       "      <td>0.004837</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>888.523786</td>\n",
       "      <td>5.315452</td>\n",
       "      <td>310.464455</td>\n",
       "      <td>0.782798</td>\n",
       "      <td>SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123)</td>\n",
       "      <td>6</td>\n",
       "      <td>ColumnTransformer(transformers=[('tf_idf_vectorizer',\\n                                 TfidfVectorizer(max_df=0.8, min_df=2,\\n                                                 ngram_range=(1, 2),\\n                                                 stop_words={'a', 'about',\\n                                                             'above', 'after',\\n                                                             'again', 'against',\\n                                                             'ain', 'all', 'am',\\n                                                             'an', 'and', 'any',\\n                                                             'april', 'are',\\n                                                             'aren', \"aren't\",\\n                                                             'as', 'at',\\n                                                             'august', 'be',\\n                                                             'because', 'been',\\n                                                             'before', 'being',\\n                                                             'below', 'between',\\n                                                             'both', 'but',\\n                                                             'by', 'can', ...}),\\n                                 'lemmatized')])</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>{'classifier': SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123), 'classifier__C': 6, 'classifier__gamma': 0.3, 'vectorizer': ColumnTransformer(transformers=[('tf_idf_vectorizer',\n",
       "                                 TfidfVectorizer(max_df=0.8, min_df=2,\n",
       "                                                 ngram_range=(1, 2),\n",
       "                                                 stop_words={'a', 'about',\n",
       "                                                             'above', 'after',\n",
       "                                                             'again', 'against',\n",
       "                                                             'ain', 'all', 'am',\n",
       "                                                             'an', 'and', 'any',\n",
       "                                                             'april', 'are',\n",
       "                                                             'aren', \"aren't\",\n",
       "                                                             'as', 'at',\n",
       "                                                             'august', 'be',\n",
       "                                                             'because', 'been',\n",
       "                                                             'before', 'being',\n",
       "                                                             'below', 'between',\n",
       "                                                             'both', 'but',\n",
       "                                                             'by', 'can', ...}),\n",
       "                                 'lemmatized')]), 'vectorizer__tf_idf_vectorizer__max_df': 0.9, 'vectorizer__tf_idf_vectorizer__min_df': 2, 'vectorizer__tf_idf_vectorizer__ngram_range': (1, 3)}</td>\n",
       "      <td>0.783587</td>\n",
       "      <td>0.774616</td>\n",
       "      <td>0.770121</td>\n",
       "      <td>0.776108</td>\n",
       "      <td>0.005598</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>893.757156</td>\n",
       "      <td>2.501904</td>\n",
       "      <td>309.159850</td>\n",
       "      <td>4.315958</td>\n",
       "      <td>SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123)</td>\n",
       "      <td>6</td>\n",
       "      <td>ColumnTransformer(transformers=[('tf_idf_vectorizer',\\n                                 TfidfVectorizer(max_df=0.8, min_df=2,\\n                                                 ngram_range=(1, 2),\\n                                                 stop_words={'a', 'about',\\n                                                             'above', 'after',\\n                                                             'again', 'against',\\n                                                             'ain', 'all', 'am',\\n                                                             'an', 'and', 'any',\\n                                                             'april', 'are',\\n                                                             'aren', \"aren't\",\\n                                                             'as', 'at',\\n                                                             'august', 'be',\\n                                                             'because', 'been',\\n                                                             'before', 'being',\\n                                                             'below', 'between',\\n                                                             'both', 'but',\\n                                                             'by', 'can', ...}),\\n                                 'lemmatized')])</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>{'classifier': SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123), 'classifier__C': 6, 'classifier__gamma': 0.3, 'vectorizer': ColumnTransformer(transformers=[('tf_idf_vectorizer',\n",
       "                                 TfidfVectorizer(max_df=0.8, min_df=2,\n",
       "                                                 ngram_range=(1, 2),\n",
       "                                                 stop_words={'a', 'about',\n",
       "                                                             'above', 'after',\n",
       "                                                             'again', 'against',\n",
       "                                                             'ain', 'all', 'am',\n",
       "                                                             'an', 'and', 'any',\n",
       "                                                             'april', 'are',\n",
       "                                                             'aren', \"aren't\",\n",
       "                                                             'as', 'at',\n",
       "                                                             'august', 'be',\n",
       "                                                             'because', 'been',\n",
       "                                                             'before', 'being',\n",
       "                                                             'below', 'between',\n",
       "                                                             'both', 'but',\n",
       "                                                             'by', 'can', ...}),\n",
       "                                 'lemmatized')]), 'vectorizer__tf_idf_vectorizer__max_df': 0.8, 'vectorizer__tf_idf_vectorizer__min_df': 2, 'vectorizer__tf_idf_vectorizer__ngram_range': (1, 3)}</td>\n",
       "      <td>0.783587</td>\n",
       "      <td>0.774616</td>\n",
       "      <td>0.770121</td>\n",
       "      <td>0.776108</td>\n",
       "      <td>0.005598</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>835.714489</td>\n",
       "      <td>6.087284</td>\n",
       "      <td>289.380812</td>\n",
       "      <td>4.711815</td>\n",
       "      <td>SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123)</td>\n",
       "      <td>6</td>\n",
       "      <td>ColumnTransformer(transformers=[('tf_idf_vectorizer',\\n                                 TfidfVectorizer(max_df=0.8, min_df=2,\\n                                                 ngram_range=(1, 2),\\n                                                 stop_words={'a', 'about',\\n                                                             'above', 'after',\\n                                                             'again', 'against',\\n                                                             'ain', 'all', 'am',\\n                                                             'an', 'and', 'any',\\n                                                             'april', 'are',\\n                                                             'aren', \"aren't\",\\n                                                             'as', 'at',\\n                                                             'august', 'be',\\n                                                             'because', 'been',\\n                                                             'before', 'being',\\n                                                             'below', 'between',\\n                                                             'both', 'but',\\n                                                             'by', 'can', ...}),\\n                                 'lemmatized')])</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>{'classifier': SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123), 'classifier__C': 6, 'classifier__gamma': 0.3, 'vectorizer': ColumnTransformer(transformers=[('tf_idf_vectorizer',\n",
       "                                 TfidfVectorizer(max_df=0.8, min_df=2,\n",
       "                                                 ngram_range=(1, 2),\n",
       "                                                 stop_words={'a', 'about',\n",
       "                                                             'above', 'after',\n",
       "                                                             'again', 'against',\n",
       "                                                             'ain', 'all', 'am',\n",
       "                                                             'an', 'and', 'any',\n",
       "                                                             'april', 'are',\n",
       "                                                             'aren', \"aren't\",\n",
       "                                                             'as', 'at',\n",
       "                                                             'august', 'be',\n",
       "                                                             'because', 'been',\n",
       "                                                             'before', 'being',\n",
       "                                                             'below', 'between',\n",
       "                                                             'both', 'but',\n",
       "                                                             'by', 'can', ...}),\n",
       "                                 'lemmatized')]), 'vectorizer__tf_idf_vectorizer__max_df': 0.8, 'vectorizer__tf_idf_vectorizer__min_df': 2, 'vectorizer__tf_idf_vectorizer__ngram_range': (1, 2)}</td>\n",
       "      <td>0.782919</td>\n",
       "      <td>0.773219</td>\n",
       "      <td>0.770512</td>\n",
       "      <td>0.775550</td>\n",
       "      <td>0.005327</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>825.246846</td>\n",
       "      <td>2.965579</td>\n",
       "      <td>294.506444</td>\n",
       "      <td>1.126151</td>\n",
       "      <td>SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123)</td>\n",
       "      <td>6</td>\n",
       "      <td>ColumnTransformer(transformers=[('tf_idf_vectorizer',\\n                                 TfidfVectorizer(max_df=0.8, min_df=2,\\n                                                 ngram_range=(1, 2),\\n                                                 stop_words={'a', 'about',\\n                                                             'above', 'after',\\n                                                             'again', 'against',\\n                                                             'ain', 'all', 'am',\\n                                                             'an', 'and', 'any',\\n                                                             'april', 'are',\\n                                                             'aren', \"aren't\",\\n                                                             'as', 'at',\\n                                                             'august', 'be',\\n                                                             'because', 'been',\\n                                                             'before', 'being',\\n                                                             'below', 'between',\\n                                                             'both', 'but',\\n                                                             'by', 'can', ...}),\\n                                 'lemmatized')])</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>{'classifier': SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123), 'classifier__C': 6, 'classifier__gamma': 0.3, 'vectorizer': ColumnTransformer(transformers=[('tf_idf_vectorizer',\n",
       "                                 TfidfVectorizer(max_df=0.8, min_df=2,\n",
       "                                                 ngram_range=(1, 2),\n",
       "                                                 stop_words={'a', 'about',\n",
       "                                                             'above', 'after',\n",
       "                                                             'again', 'against',\n",
       "                                                             'ain', 'all', 'am',\n",
       "                                                             'an', 'and', 'any',\n",
       "                                                             'april', 'are',\n",
       "                                                             'aren', \"aren't\",\n",
       "                                                             'as', 'at',\n",
       "                                                             'august', 'be',\n",
       "                                                             'because', 'been',\n",
       "                                                             'before', 'being',\n",
       "                                                             'below', 'between',\n",
       "                                                             'both', 'but',\n",
       "                                                             'by', 'can', ...}),\n",
       "                                 'lemmatized')]), 'vectorizer__tf_idf_vectorizer__max_df': 0.9, 'vectorizer__tf_idf_vectorizer__min_df': 2, 'vectorizer__tf_idf_vectorizer__ngram_range': (1, 2)}</td>\n",
       "      <td>0.782919</td>\n",
       "      <td>0.773219</td>\n",
       "      <td>0.770512</td>\n",
       "      <td>0.775550</td>\n",
       "      <td>0.005327</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>877.678931</td>\n",
       "      <td>6.353533</td>\n",
       "      <td>324.577345</td>\n",
       "      <td>7.271551</td>\n",
       "      <td>SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123)</td>\n",
       "      <td>10</td>\n",
       "      <td>ColumnTransformer(transformers=[('tf_idf_vectorizer',\\n                                 TfidfVectorizer(max_df=0.8, min_df=2,\\n                                                 ngram_range=(1, 2),\\n                                                 stop_words={'a', 'about',\\n                                                             'above', 'after',\\n                                                             'again', 'against',\\n                                                             'ain', 'all', 'am',\\n                                                             'an', 'and', 'any',\\n                                                             'april', 'are',\\n                                                             'aren', \"aren't\",\\n                                                             'as', 'at',\\n                                                             'august', 'be',\\n                                                             'because', 'been',\\n                                                             'before', 'being',\\n                                                             'below', 'between',\\n                                                             'both', 'but',\\n                                                             'by', 'can', ...}),\\n                                 'lemmatized')])</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>{'classifier': SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123), 'classifier__C': 10, 'classifier__gamma': 0.3, 'vectorizer': ColumnTransformer(transformers=[('tf_idf_vectorizer',\n",
       "                                 TfidfVectorizer(max_df=0.8, min_df=2,\n",
       "                                                 ngram_range=(1, 2),\n",
       "                                                 stop_words={'a', 'about',\n",
       "                                                             'above', 'after',\n",
       "                                                             'again', 'against',\n",
       "                                                             'ain', 'all', 'am',\n",
       "                                                             'an', 'and', 'any',\n",
       "                                                             'april', 'are',\n",
       "                                                             'aren', \"aren't\",\n",
       "                                                             'as', 'at',\n",
       "                                                             'august', 'be',\n",
       "                                                             'because', 'been',\n",
       "                                                             'before', 'being',\n",
       "                                                             'below', 'between',\n",
       "                                                             'both', 'but',\n",
       "                                                             'by', 'can', ...}),\n",
       "                                 'lemmatized')]), 'vectorizer__tf_idf_vectorizer__max_df': 0.8, 'vectorizer__tf_idf_vectorizer__min_df': 2, 'vectorizer__tf_idf_vectorizer__ngram_range': (1, 3)}</td>\n",
       "      <td>0.781441</td>\n",
       "      <td>0.774801</td>\n",
       "      <td>0.768480</td>\n",
       "      <td>0.774907</td>\n",
       "      <td>0.005292</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>884.746247</td>\n",
       "      <td>3.822683</td>\n",
       "      <td>309.112449</td>\n",
       "      <td>4.597257</td>\n",
       "      <td>SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123)</td>\n",
       "      <td>10</td>\n",
       "      <td>ColumnTransformer(transformers=[('tf_idf_vectorizer',\\n                                 TfidfVectorizer(max_df=0.8, min_df=2,\\n                                                 ngram_range=(1, 2),\\n                                                 stop_words={'a', 'about',\\n                                                             'above', 'after',\\n                                                             'again', 'against',\\n                                                             'ain', 'all', 'am',\\n                                                             'an', 'and', 'any',\\n                                                             'april', 'are',\\n                                                             'aren', \"aren't\",\\n                                                             'as', 'at',\\n                                                             'august', 'be',\\n                                                             'because', 'been',\\n                                                             'before', 'being',\\n                                                             'below', 'between',\\n                                                             'both', 'but',\\n                                                             'by', 'can', ...}),\\n                                 'lemmatized')])</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>{'classifier': SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123), 'classifier__C': 10, 'classifier__gamma': 0.3, 'vectorizer': ColumnTransformer(transformers=[('tf_idf_vectorizer',\n",
       "                                 TfidfVectorizer(max_df=0.8, min_df=2,\n",
       "                                                 ngram_range=(1, 2),\n",
       "                                                 stop_words={'a', 'about',\n",
       "                                                             'above', 'after',\n",
       "                                                             'again', 'against',\n",
       "                                                             'ain', 'all', 'am',\n",
       "                                                             'an', 'and', 'any',\n",
       "                                                             'april', 'are',\n",
       "                                                             'aren', \"aren't\",\n",
       "                                                             'as', 'at',\n",
       "                                                             'august', 'be',\n",
       "                                                             'because', 'been',\n",
       "                                                             'before', 'being',\n",
       "                                                             'below', 'between',\n",
       "                                                             'both', 'but',\n",
       "                                                             'by', 'can', ...}),\n",
       "                                 'lemmatized')]), 'vectorizer__tf_idf_vectorizer__max_df': 0.9, 'vectorizer__tf_idf_vectorizer__min_df': 2, 'vectorizer__tf_idf_vectorizer__ngram_range': (1, 3)}</td>\n",
       "      <td>0.781441</td>\n",
       "      <td>0.774801</td>\n",
       "      <td>0.768480</td>\n",
       "      <td>0.774907</td>\n",
       "      <td>0.005292</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     839.312081      3.841229       297.256259        3.598656   \n",
       "1     840.153359      3.484650       297.419488        2.379613   \n",
       "2     907.706812      2.957971       315.612825        2.884368   \n",
       "3     903.359438      6.877142       316.130112        2.778566   \n",
       "4     888.523786      5.315452       310.464455        0.782798   \n",
       "5     893.757156      2.501904       309.159850        4.315958   \n",
       "6     835.714489      6.087284       289.380812        4.711815   \n",
       "7     825.246846      2.965579       294.506444        1.126151   \n",
       "8     877.678931      6.353533       324.577345        7.271551   \n",
       "9     884.746247      3.822683       309.112449        4.597257   \n",
       "\n",
       "                                                                 param_classifier  \\\n",
       "0  SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123)   \n",
       "1  SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123)   \n",
       "2  SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123)   \n",
       "3  SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123)   \n",
       "4  SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123)   \n",
       "5  SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123)   \n",
       "6  SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123)   \n",
       "7  SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123)   \n",
       "8  SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123)   \n",
       "9  SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123)   \n",
       "\n",
       "  param_classifier__C  \\\n",
       "0                   3   \n",
       "1                   3   \n",
       "2                   3   \n",
       "3                   3   \n",
       "4                   6   \n",
       "5                   6   \n",
       "6                   6   \n",
       "7                   6   \n",
       "8                  10   \n",
       "9                  10   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   param_vectorizer  \\\n",
       "0  ColumnTransformer(transformers=[('tf_idf_vectorizer',\\n                                 TfidfVectorizer(max_df=0.8, min_df=2,\\n                                                 ngram_range=(1, 2),\\n                                                 stop_words={'a', 'about',\\n                                                             'above', 'after',\\n                                                             'again', 'against',\\n                                                             'ain', 'all', 'am',\\n                                                             'an', 'and', 'any',\\n                                                             'april', 'are',\\n                                                             'aren', \"aren't\",\\n                                                             'as', 'at',\\n                                                             'august', 'be',\\n                                                             'because', 'been',\\n                                                             'before', 'being',\\n                                                             'below', 'between',\\n                                                             'both', 'but',\\n                                                             'by', 'can', ...}),\\n                                 'lemmatized')])   \n",
       "1  ColumnTransformer(transformers=[('tf_idf_vectorizer',\\n                                 TfidfVectorizer(max_df=0.8, min_df=2,\\n                                                 ngram_range=(1, 2),\\n                                                 stop_words={'a', 'about',\\n                                                             'above', 'after',\\n                                                             'again', 'against',\\n                                                             'ain', 'all', 'am',\\n                                                             'an', 'and', 'any',\\n                                                             'april', 'are',\\n                                                             'aren', \"aren't\",\\n                                                             'as', 'at',\\n                                                             'august', 'be',\\n                                                             'because', 'been',\\n                                                             'before', 'being',\\n                                                             'below', 'between',\\n                                                             'both', 'but',\\n                                                             'by', 'can', ...}),\\n                                 'lemmatized')])   \n",
       "2  ColumnTransformer(transformers=[('tf_idf_vectorizer',\\n                                 TfidfVectorizer(max_df=0.8, min_df=2,\\n                                                 ngram_range=(1, 2),\\n                                                 stop_words={'a', 'about',\\n                                                             'above', 'after',\\n                                                             'again', 'against',\\n                                                             'ain', 'all', 'am',\\n                                                             'an', 'and', 'any',\\n                                                             'april', 'are',\\n                                                             'aren', \"aren't\",\\n                                                             'as', 'at',\\n                                                             'august', 'be',\\n                                                             'because', 'been',\\n                                                             'before', 'being',\\n                                                             'below', 'between',\\n                                                             'both', 'but',\\n                                                             'by', 'can', ...}),\\n                                 'lemmatized')])   \n",
       "3  ColumnTransformer(transformers=[('tf_idf_vectorizer',\\n                                 TfidfVectorizer(max_df=0.8, min_df=2,\\n                                                 ngram_range=(1, 2),\\n                                                 stop_words={'a', 'about',\\n                                                             'above', 'after',\\n                                                             'again', 'against',\\n                                                             'ain', 'all', 'am',\\n                                                             'an', 'and', 'any',\\n                                                             'april', 'are',\\n                                                             'aren', \"aren't\",\\n                                                             'as', 'at',\\n                                                             'august', 'be',\\n                                                             'because', 'been',\\n                                                             'before', 'being',\\n                                                             'below', 'between',\\n                                                             'both', 'but',\\n                                                             'by', 'can', ...}),\\n                                 'lemmatized')])   \n",
       "4  ColumnTransformer(transformers=[('tf_idf_vectorizer',\\n                                 TfidfVectorizer(max_df=0.8, min_df=2,\\n                                                 ngram_range=(1, 2),\\n                                                 stop_words={'a', 'about',\\n                                                             'above', 'after',\\n                                                             'again', 'against',\\n                                                             'ain', 'all', 'am',\\n                                                             'an', 'and', 'any',\\n                                                             'april', 'are',\\n                                                             'aren', \"aren't\",\\n                                                             'as', 'at',\\n                                                             'august', 'be',\\n                                                             'because', 'been',\\n                                                             'before', 'being',\\n                                                             'below', 'between',\\n                                                             'both', 'but',\\n                                                             'by', 'can', ...}),\\n                                 'lemmatized')])   \n",
       "5  ColumnTransformer(transformers=[('tf_idf_vectorizer',\\n                                 TfidfVectorizer(max_df=0.8, min_df=2,\\n                                                 ngram_range=(1, 2),\\n                                                 stop_words={'a', 'about',\\n                                                             'above', 'after',\\n                                                             'again', 'against',\\n                                                             'ain', 'all', 'am',\\n                                                             'an', 'and', 'any',\\n                                                             'april', 'are',\\n                                                             'aren', \"aren't\",\\n                                                             'as', 'at',\\n                                                             'august', 'be',\\n                                                             'because', 'been',\\n                                                             'before', 'being',\\n                                                             'below', 'between',\\n                                                             'both', 'but',\\n                                                             'by', 'can', ...}),\\n                                 'lemmatized')])   \n",
       "6  ColumnTransformer(transformers=[('tf_idf_vectorizer',\\n                                 TfidfVectorizer(max_df=0.8, min_df=2,\\n                                                 ngram_range=(1, 2),\\n                                                 stop_words={'a', 'about',\\n                                                             'above', 'after',\\n                                                             'again', 'against',\\n                                                             'ain', 'all', 'am',\\n                                                             'an', 'and', 'any',\\n                                                             'april', 'are',\\n                                                             'aren', \"aren't\",\\n                                                             'as', 'at',\\n                                                             'august', 'be',\\n                                                             'because', 'been',\\n                                                             'before', 'being',\\n                                                             'below', 'between',\\n                                                             'both', 'but',\\n                                                             'by', 'can', ...}),\\n                                 'lemmatized')])   \n",
       "7  ColumnTransformer(transformers=[('tf_idf_vectorizer',\\n                                 TfidfVectorizer(max_df=0.8, min_df=2,\\n                                                 ngram_range=(1, 2),\\n                                                 stop_words={'a', 'about',\\n                                                             'above', 'after',\\n                                                             'again', 'against',\\n                                                             'ain', 'all', 'am',\\n                                                             'an', 'and', 'any',\\n                                                             'april', 'are',\\n                                                             'aren', \"aren't\",\\n                                                             'as', 'at',\\n                                                             'august', 'be',\\n                                                             'because', 'been',\\n                                                             'before', 'being',\\n                                                             'below', 'between',\\n                                                             'both', 'but',\\n                                                             'by', 'can', ...}),\\n                                 'lemmatized')])   \n",
       "8  ColumnTransformer(transformers=[('tf_idf_vectorizer',\\n                                 TfidfVectorizer(max_df=0.8, min_df=2,\\n                                                 ngram_range=(1, 2),\\n                                                 stop_words={'a', 'about',\\n                                                             'above', 'after',\\n                                                             'again', 'against',\\n                                                             'ain', 'all', 'am',\\n                                                             'an', 'and', 'any',\\n                                                             'april', 'are',\\n                                                             'aren', \"aren't\",\\n                                                             'as', 'at',\\n                                                             'august', 'be',\\n                                                             'because', 'been',\\n                                                             'before', 'being',\\n                                                             'below', 'between',\\n                                                             'both', 'but',\\n                                                             'by', 'can', ...}),\\n                                 'lemmatized')])   \n",
       "9  ColumnTransformer(transformers=[('tf_idf_vectorizer',\\n                                 TfidfVectorizer(max_df=0.8, min_df=2,\\n                                                 ngram_range=(1, 2),\\n                                                 stop_words={'a', 'about',\\n                                                             'above', 'after',\\n                                                             'again', 'against',\\n                                                             'ain', 'all', 'am',\\n                                                             'an', 'and', 'any',\\n                                                             'april', 'are',\\n                                                             'aren', \"aren't\",\\n                                                             'as', 'at',\\n                                                             'august', 'be',\\n                                                             'because', 'been',\\n                                                             'before', 'being',\\n                                                             'below', 'between',\\n                                                             'both', 'but',\\n                                                             'by', 'can', ...}),\\n                                 'lemmatized')])   \n",
       "\n",
       "  param_vectorizer__tf_idf_vectorizer__ngram_range param_classifier__gamma  \\\n",
       "0                                           (1, 2)                     0.3   \n",
       "1                                           (1, 2)                     0.3   \n",
       "2                                           (1, 3)                     0.3   \n",
       "3                                           (1, 3)                     0.3   \n",
       "4                                           (1, 3)                     0.3   \n",
       "5                                           (1, 3)                     0.3   \n",
       "6                                           (1, 2)                     0.3   \n",
       "7                                           (1, 2)                     0.3   \n",
       "8                                           (1, 3)                     0.3   \n",
       "9                                           (1, 3)                     0.3   \n",
       "\n",
       "  param_vectorizer__tf_idf_vectorizer__max_df  \\\n",
       "0                                         0.9   \n",
       "1                                         0.8   \n",
       "2                                         0.8   \n",
       "3                                         0.9   \n",
       "4                                         0.9   \n",
       "5                                         0.8   \n",
       "6                                         0.8   \n",
       "7                                         0.9   \n",
       "8                                         0.8   \n",
       "9                                         0.9   \n",
       "\n",
       "  param_vectorizer__tf_idf_vectorizer__min_df  \\\n",
       "0                                           2   \n",
       "1                                           2   \n",
       "2                                           2   \n",
       "3                                           2   \n",
       "4                                           2   \n",
       "5                                           2   \n",
       "6                                           2   \n",
       "7                                           2   \n",
       "8                                           2   \n",
       "9                                           2   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         params  \\\n",
       "0   {'classifier': SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123), 'classifier__C': 3, 'classifier__gamma': 0.3, 'vectorizer': ColumnTransformer(transformers=[('tf_idf_vectorizer',\n",
       "                                 TfidfVectorizer(max_df=0.8, min_df=2,\n",
       "                                                 ngram_range=(1, 2),\n",
       "                                                 stop_words={'a', 'about',\n",
       "                                                             'above', 'after',\n",
       "                                                             'again', 'against',\n",
       "                                                             'ain', 'all', 'am',\n",
       "                                                             'an', 'and', 'any',\n",
       "                                                             'april', 'are',\n",
       "                                                             'aren', \"aren't\",\n",
       "                                                             'as', 'at',\n",
       "                                                             'august', 'be',\n",
       "                                                             'because', 'been',\n",
       "                                                             'before', 'being',\n",
       "                                                             'below', 'between',\n",
       "                                                             'both', 'but',\n",
       "                                                             'by', 'can', ...}),\n",
       "                                 'lemmatized')]), 'vectorizer__tf_idf_vectorizer__max_df': 0.9, 'vectorizer__tf_idf_vectorizer__min_df': 2, 'vectorizer__tf_idf_vectorizer__ngram_range': (1, 2)}   \n",
       "1   {'classifier': SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123), 'classifier__C': 3, 'classifier__gamma': 0.3, 'vectorizer': ColumnTransformer(transformers=[('tf_idf_vectorizer',\n",
       "                                 TfidfVectorizer(max_df=0.8, min_df=2,\n",
       "                                                 ngram_range=(1, 2),\n",
       "                                                 stop_words={'a', 'about',\n",
       "                                                             'above', 'after',\n",
       "                                                             'again', 'against',\n",
       "                                                             'ain', 'all', 'am',\n",
       "                                                             'an', 'and', 'any',\n",
       "                                                             'april', 'are',\n",
       "                                                             'aren', \"aren't\",\n",
       "                                                             'as', 'at',\n",
       "                                                             'august', 'be',\n",
       "                                                             'because', 'been',\n",
       "                                                             'before', 'being',\n",
       "                                                             'below', 'between',\n",
       "                                                             'both', 'but',\n",
       "                                                             'by', 'can', ...}),\n",
       "                                 'lemmatized')]), 'vectorizer__tf_idf_vectorizer__max_df': 0.8, 'vectorizer__tf_idf_vectorizer__min_df': 2, 'vectorizer__tf_idf_vectorizer__ngram_range': (1, 2)}   \n",
       "2   {'classifier': SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123), 'classifier__C': 3, 'classifier__gamma': 0.3, 'vectorizer': ColumnTransformer(transformers=[('tf_idf_vectorizer',\n",
       "                                 TfidfVectorizer(max_df=0.8, min_df=2,\n",
       "                                                 ngram_range=(1, 2),\n",
       "                                                 stop_words={'a', 'about',\n",
       "                                                             'above', 'after',\n",
       "                                                             'again', 'against',\n",
       "                                                             'ain', 'all', 'am',\n",
       "                                                             'an', 'and', 'any',\n",
       "                                                             'april', 'are',\n",
       "                                                             'aren', \"aren't\",\n",
       "                                                             'as', 'at',\n",
       "                                                             'august', 'be',\n",
       "                                                             'because', 'been',\n",
       "                                                             'before', 'being',\n",
       "                                                             'below', 'between',\n",
       "                                                             'both', 'but',\n",
       "                                                             'by', 'can', ...}),\n",
       "                                 'lemmatized')]), 'vectorizer__tf_idf_vectorizer__max_df': 0.8, 'vectorizer__tf_idf_vectorizer__min_df': 2, 'vectorizer__tf_idf_vectorizer__ngram_range': (1, 3)}   \n",
       "3   {'classifier': SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123), 'classifier__C': 3, 'classifier__gamma': 0.3, 'vectorizer': ColumnTransformer(transformers=[('tf_idf_vectorizer',\n",
       "                                 TfidfVectorizer(max_df=0.8, min_df=2,\n",
       "                                                 ngram_range=(1, 2),\n",
       "                                                 stop_words={'a', 'about',\n",
       "                                                             'above', 'after',\n",
       "                                                             'again', 'against',\n",
       "                                                             'ain', 'all', 'am',\n",
       "                                                             'an', 'and', 'any',\n",
       "                                                             'april', 'are',\n",
       "                                                             'aren', \"aren't\",\n",
       "                                                             'as', 'at',\n",
       "                                                             'august', 'be',\n",
       "                                                             'because', 'been',\n",
       "                                                             'before', 'being',\n",
       "                                                             'below', 'between',\n",
       "                                                             'both', 'but',\n",
       "                                                             'by', 'can', ...}),\n",
       "                                 'lemmatized')]), 'vectorizer__tf_idf_vectorizer__max_df': 0.9, 'vectorizer__tf_idf_vectorizer__min_df': 2, 'vectorizer__tf_idf_vectorizer__ngram_range': (1, 3)}   \n",
       "4   {'classifier': SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123), 'classifier__C': 6, 'classifier__gamma': 0.3, 'vectorizer': ColumnTransformer(transformers=[('tf_idf_vectorizer',\n",
       "                                 TfidfVectorizer(max_df=0.8, min_df=2,\n",
       "                                                 ngram_range=(1, 2),\n",
       "                                                 stop_words={'a', 'about',\n",
       "                                                             'above', 'after',\n",
       "                                                             'again', 'against',\n",
       "                                                             'ain', 'all', 'am',\n",
       "                                                             'an', 'and', 'any',\n",
       "                                                             'april', 'are',\n",
       "                                                             'aren', \"aren't\",\n",
       "                                                             'as', 'at',\n",
       "                                                             'august', 'be',\n",
       "                                                             'because', 'been',\n",
       "                                                             'before', 'being',\n",
       "                                                             'below', 'between',\n",
       "                                                             'both', 'but',\n",
       "                                                             'by', 'can', ...}),\n",
       "                                 'lemmatized')]), 'vectorizer__tf_idf_vectorizer__max_df': 0.9, 'vectorizer__tf_idf_vectorizer__min_df': 2, 'vectorizer__tf_idf_vectorizer__ngram_range': (1, 3)}   \n",
       "5   {'classifier': SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123), 'classifier__C': 6, 'classifier__gamma': 0.3, 'vectorizer': ColumnTransformer(transformers=[('tf_idf_vectorizer',\n",
       "                                 TfidfVectorizer(max_df=0.8, min_df=2,\n",
       "                                                 ngram_range=(1, 2),\n",
       "                                                 stop_words={'a', 'about',\n",
       "                                                             'above', 'after',\n",
       "                                                             'again', 'against',\n",
       "                                                             'ain', 'all', 'am',\n",
       "                                                             'an', 'and', 'any',\n",
       "                                                             'april', 'are',\n",
       "                                                             'aren', \"aren't\",\n",
       "                                                             'as', 'at',\n",
       "                                                             'august', 'be',\n",
       "                                                             'because', 'been',\n",
       "                                                             'before', 'being',\n",
       "                                                             'below', 'between',\n",
       "                                                             'both', 'but',\n",
       "                                                             'by', 'can', ...}),\n",
       "                                 'lemmatized')]), 'vectorizer__tf_idf_vectorizer__max_df': 0.8, 'vectorizer__tf_idf_vectorizer__min_df': 2, 'vectorizer__tf_idf_vectorizer__ngram_range': (1, 3)}   \n",
       "6   {'classifier': SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123), 'classifier__C': 6, 'classifier__gamma': 0.3, 'vectorizer': ColumnTransformer(transformers=[('tf_idf_vectorizer',\n",
       "                                 TfidfVectorizer(max_df=0.8, min_df=2,\n",
       "                                                 ngram_range=(1, 2),\n",
       "                                                 stop_words={'a', 'about',\n",
       "                                                             'above', 'after',\n",
       "                                                             'again', 'against',\n",
       "                                                             'ain', 'all', 'am',\n",
       "                                                             'an', 'and', 'any',\n",
       "                                                             'april', 'are',\n",
       "                                                             'aren', \"aren't\",\n",
       "                                                             'as', 'at',\n",
       "                                                             'august', 'be',\n",
       "                                                             'because', 'been',\n",
       "                                                             'before', 'being',\n",
       "                                                             'below', 'between',\n",
       "                                                             'both', 'but',\n",
       "                                                             'by', 'can', ...}),\n",
       "                                 'lemmatized')]), 'vectorizer__tf_idf_vectorizer__max_df': 0.8, 'vectorizer__tf_idf_vectorizer__min_df': 2, 'vectorizer__tf_idf_vectorizer__ngram_range': (1, 2)}   \n",
       "7   {'classifier': SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123), 'classifier__C': 6, 'classifier__gamma': 0.3, 'vectorizer': ColumnTransformer(transformers=[('tf_idf_vectorizer',\n",
       "                                 TfidfVectorizer(max_df=0.8, min_df=2,\n",
       "                                                 ngram_range=(1, 2),\n",
       "                                                 stop_words={'a', 'about',\n",
       "                                                             'above', 'after',\n",
       "                                                             'again', 'against',\n",
       "                                                             'ain', 'all', 'am',\n",
       "                                                             'an', 'and', 'any',\n",
       "                                                             'april', 'are',\n",
       "                                                             'aren', \"aren't\",\n",
       "                                                             'as', 'at',\n",
       "                                                             'august', 'be',\n",
       "                                                             'because', 'been',\n",
       "                                                             'before', 'being',\n",
       "                                                             'below', 'between',\n",
       "                                                             'both', 'but',\n",
       "                                                             'by', 'can', ...}),\n",
       "                                 'lemmatized')]), 'vectorizer__tf_idf_vectorizer__max_df': 0.9, 'vectorizer__tf_idf_vectorizer__min_df': 2, 'vectorizer__tf_idf_vectorizer__ngram_range': (1, 2)}   \n",
       "8  {'classifier': SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123), 'classifier__C': 10, 'classifier__gamma': 0.3, 'vectorizer': ColumnTransformer(transformers=[('tf_idf_vectorizer',\n",
       "                                 TfidfVectorizer(max_df=0.8, min_df=2,\n",
       "                                                 ngram_range=(1, 2),\n",
       "                                                 stop_words={'a', 'about',\n",
       "                                                             'above', 'after',\n",
       "                                                             'again', 'against',\n",
       "                                                             'ain', 'all', 'am',\n",
       "                                                             'an', 'and', 'any',\n",
       "                                                             'april', 'are',\n",
       "                                                             'aren', \"aren't\",\n",
       "                                                             'as', 'at',\n",
       "                                                             'august', 'be',\n",
       "                                                             'because', 'been',\n",
       "                                                             'before', 'being',\n",
       "                                                             'below', 'between',\n",
       "                                                             'both', 'but',\n",
       "                                                             'by', 'can', ...}),\n",
       "                                 'lemmatized')]), 'vectorizer__tf_idf_vectorizer__max_df': 0.8, 'vectorizer__tf_idf_vectorizer__min_df': 2, 'vectorizer__tf_idf_vectorizer__ngram_range': (1, 3)}   \n",
       "9  {'classifier': SVC(C=3, class_weight='balanced', gamma=0.3, max_iter=10000, random_state=123), 'classifier__C': 10, 'classifier__gamma': 0.3, 'vectorizer': ColumnTransformer(transformers=[('tf_idf_vectorizer',\n",
       "                                 TfidfVectorizer(max_df=0.8, min_df=2,\n",
       "                                                 ngram_range=(1, 2),\n",
       "                                                 stop_words={'a', 'about',\n",
       "                                                             'above', 'after',\n",
       "                                                             'again', 'against',\n",
       "                                                             'ain', 'all', 'am',\n",
       "                                                             'an', 'and', 'any',\n",
       "                                                             'april', 'are',\n",
       "                                                             'aren', \"aren't\",\n",
       "                                                             'as', 'at',\n",
       "                                                             'august', 'be',\n",
       "                                                             'because', 'been',\n",
       "                                                             'before', 'being',\n",
       "                                                             'below', 'between',\n",
       "                                                             'both', 'but',\n",
       "                                                             'by', 'can', ...}),\n",
       "                                 'lemmatized')]), 'vectorizer__tf_idf_vectorizer__max_df': 0.9, 'vectorizer__tf_idf_vectorizer__min_df': 2, 'vectorizer__tf_idf_vectorizer__ngram_range': (1, 3)}   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  mean_test_score  \\\n",
       "0           0.787381           0.779184           0.774892         0.780486   \n",
       "1           0.787381           0.779184           0.774892         0.780486   \n",
       "2           0.786163           0.777389           0.774879         0.779477   \n",
       "3           0.786163           0.777389           0.774879         0.779477   \n",
       "4           0.783587           0.774616           0.770121         0.776108   \n",
       "5           0.783587           0.774616           0.770121         0.776108   \n",
       "6           0.782919           0.773219           0.770512         0.775550   \n",
       "7           0.782919           0.773219           0.770512         0.775550   \n",
       "8           0.781441           0.774801           0.768480         0.774907   \n",
       "9           0.781441           0.774801           0.768480         0.774907   \n",
       "\n",
       "   std_test_score  rank_test_score  \n",
       "0        0.005181                1  \n",
       "1        0.005181                1  \n",
       "2        0.004837                3  \n",
       "3        0.004837                3  \n",
       "4        0.005598                5  \n",
       "5        0.005598                5  \n",
       "6        0.005327                7  \n",
       "7        0.005327                7  \n",
       "8        0.005292                9  \n",
       "9        0.005292                9  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(multi_gs_res).sort_values(by='mean_test_score', ascending=False).reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним лучшую модель и оценим ее точность на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 лучшей модели по версии GridSearchCV: 0.776\n",
      "Wall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('F1 лучшей модели по версии GridSearchCV: {:.3f}'.format(f1_score(test_target, multi_gs.predict(test_features))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На тестовой выборке модель показала результат не хуже, чем на кросс-валидации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск наилучших гиперпараметров при помощи Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-02 02:30:27,312]\u001b[0m A new study created in memory with name: no-name-fa8010e7-e70a-4be3-a46c-f3fdf5bca3aa\u001b[0m\n",
      "C:\\anaconda3\\lib\\site-packages\\optuna\\study\\study.py:393: FutureWarning: `n_jobs` argument has been deprecated in v2.7.0. This feature will be removed in v4.0.0. See https://github.com/optuna/optuna/releases/tag/v2.7.0.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-05-02 02:37:58,908]\u001b[0m Trial 0 finished with value: 0.7756004104434996 and parameters: {'vectorizer': 'tf_idf_ct', 'estimator': 'svm_clf', 'C': 4.480535885117198, 'gamma': 0.8323096794508871}. Best is trial 0 with value: 0.7756004104434996.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    vectorizers = trial.suggest_categorical('vectorizer', ['bow_ct', 'tf_idf_ct'])\n",
    "    if vectorizers == 'bow_ct':\n",
    "        vectorizer = ColumnTransformer(transformers=[('count_vectorizer', CountVectorizer(stop_words=stop_words), 'lemmatized')])\n",
    "    elif vectorizers == 'tf_idf_ct':\n",
    "        vectorizer =  ColumnTransformer(transformers=[('tf_idf_vectorizer', TfidfVectorizer(stop_words=stop_words), 'lemmatized')])\n",
    "    \n",
    "    estimators = trial.suggest_categorical('estimator', ['lr_clf', 'svm_clf'])\n",
    "    if estimators == 'lr_clf':\n",
    "        C = trial.suggest_float('C', 0.1, 10)\n",
    "        clf = LogisticRegression(class_weight='balanced', random_state=123, solver='sag', max_iter=10000, n_jobs=-1, C=C)\n",
    "    elif estimators == 'svm_clf':\n",
    "        C = trial.suggest_float('C', 0.1, 10)\n",
    "        gamma = trial.suggest_float('gamma', 0.1, 10)\n",
    "        clf = SVC(class_weight='balanced', random_state=123, max_iter=10000, C=C, gamma=gamma)\n",
    "    \n",
    "    pipe = Pipeline(steps=[('vectorizer', vectorizer), ('classifier', clf)])\n",
    "    \n",
    "    res = cross_val_score(pipe, train_features, train_target, scoring='f1', cv=3, n_jobs=-1, error_score='raise').mean()\n",
    "    \n",
    "    return res\n",
    "        \n",
    "study = optuna.create_study(direction='maximize', sampler=RandomSampler(seed=123))\n",
    "study.optimize(objective, n_trials=1, n_jobs=-1, timeout=4*60*60)\n",
    "\n",
    "trial = study.best_trial    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем параметры лучшей модели и сохраним ее для расчета на тестовой выборке "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectorizer': 'tf_idf_ct',\n",
       " 'estimator': 'svm_clf',\n",
       " 'C': 4.480535885117198,\n",
       " 'gamma': 0.8323096794508871}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective(trial):\n",
    "   \n",
    "    vectorizers = trial.suggest_categorical('vectorizer', ['bow_ct', 'tf_idf_ct'])\n",
    "    if vectorizers == 'bow_ct':\n",
    "        vectorizer = ColumnTransformer(transformers=[('count_vectorizer', CountVectorizer(stop_words=stop_words), 'lemmatized')])\n",
    "    elif vectorizers == 'tf_idf_ct':\n",
    "        vectorizer =  ColumnTransformer(transformers=[('tf_idf_vectorizer', TfidfVectorizer(stop_words=stop_words), 'lemmatized')])\n",
    "    \n",
    "    estimators = trial.suggest_categorical('estimator', ['lr_clf', 'svm_clf'])\n",
    "    if estimators == 'lr_clf':\n",
    "        C = trial.suggest_float('C', 0.1, 10)\n",
    "        clf = LogisticRegression(class_weight='balanced', random_state=123, solver='sag', max_iter=10000, n_jobs=-1, C=C)\n",
    "    elif estimators == 'svm_clf':\n",
    "        C = trial.suggest_float('C', 0.1, 10)\n",
    "        gamma = trial.suggest_float('gamma', 0.1, 10)\n",
    "        clf = SVC(class_weight='balanced', random_state=123, max_iter=10000, C=C, gamma=gamma)\n",
    "    \n",
    "    pipe = Pipeline(steps=[('vectorizer', vectorizer), ('classifier', clf)])\n",
    "    \n",
    "    pipe.fit(train_features, train_target)\n",
    "    res = f1_score(test_target, pipe.predict(test_features))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 лучшей модели по версии Optuna: 0.794\n",
      "Wall time: 9min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('F1 лучшей модели по версии Optuna: {:.3f}'.format(detailed_objective(study.best_trial)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Лемматизатор библиотеки Spacy лучше справляется с определением словарной формы английских слов\n",
    "* Векторное преобразование по принципу TF-IDF эффективнее, чем bag of words\n",
    "* Среди моделей были рассмотрены линейные модели: логистическая регрессия и метод опорных векторов с регуляризацией. Градиентный бустинг требует слишком большого времени для обучения на таком количестве признаков\n",
    "* В процессе обучения было выявлено переобучение, от которого можно было бы избавиться более тщательной очисткой текста"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "463.825px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
